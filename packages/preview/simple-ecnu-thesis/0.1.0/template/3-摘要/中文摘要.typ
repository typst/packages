#import "mod.typ": *
#show: style
= 摘#h(2em)要

  公平性和可解释性一直是可信人工智能领域的重要研究热点。
  随着机器学习在现实决策系统中的应用日益广泛，
  针对涉及人类的任务，
  其公平性和可解释性的可信度尚未得到充分保障。
  主要存在以下几个问题：
  首先，
  数据本身的偏见或算法歧视可能导致不公平的决策，
  严重影响个体的机会；
  其次，
  机器学习系统的复杂性使得合理解释其不公平决策变得困难；
  第三，
  当前公平性方法中基于可解释性理论的研究相对较少，
  大多数去偏方法主要集中于评估个体公平性和群体公平性，
  而从可解释性角度进行评估的研究则显得较为稀缺，
  导致公平性方法在缓解偏见时，
  无法提供可解释的原因。

  为了解决上述问题，
  本文提出了一种基于特征贡献的可解释公平性框架，
  旨在提高二分类任务中公平性的可解释程度。
  该框架基于以下公平性假设：
  “同一群体内个体的保护属性应对其结果产生相似的贡献”。
  首先，
  针对公平性贡献，
  采用可解释的特征重要性进行解释，
  并利用博弈论中的 Shapley 值概念对该贡献进行量化；
  随后，
  根据不同的保护属性划分群体，
  并针对特定群体，
  采用相关算法实施歧视检测与去偏，
  以减轻原始样本中的偏见。

  本文贡献包括三个方面，
  第一，
  针对分类样本提出了一种基于特征贡献的歧视检测算法。
  该算法通过特征贡献和保护属性进行筛选与检测，
  识别出歧视严重和优待过度的样本，
  并输出候选集；
  第二，
  针对这些候选集提出了一种基于特征贡献的歧视消除算法。
  该算法聚焦于候选集中模型输出差异较大的样本，
  生成相似样本对，
  以缓解数据集的不平衡；
  第三，
  通过多角度的可解释方法对不同阶段的结果进行解释分析，
  并采用多种公平性指标和准确性指标进行评估。

  实验结果表明，
  所提出的方法在可解释性方面显著优于现有方法，
  并对不同分类器和公平性指标展现了广泛的适用性。
  相较于其他公平性算法，
  本文提出的算法在保证准确性的情况下，
  保证了良好的群体公平性指标，
  并提高了可解释性，
  使得能够从可解释的角度分析导致结果不公平的原因。
  本文为可信人工智能提供了一种安全且可解释的公平性方法，
  结合可解释方法多角度地在不同阶段进行可视化解释。

#v(2em)

#text(font: 黑体, size: 四号, weight: "semibold", "关键词:")#h(.5em)
关键词一，关键词二，关键词三
