@misc{liu_survey_2024,
	title = {A {Survey} of {NL2SQL} with {Large} {Language} {Models}: {Where} are we, and where are we going?},
	shorttitle = {A {Survey} of {NL2SQL} with {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2408.05109},
	doi = {10.48550/arXiv.2408.05109},
	abstract = {Translating users' natural language queries (NL) into SQL queries (i.e., NL2SQL) can significantly reduce barriers to accessing relational databases and support various commercial applications. The performance of NL2SQL has been greatly enhanced with the emergence of Large Language Models (LLMs). In this survey, we provide a comprehensive review of NL2SQL techniques powered by LLMs, covering its entire lifecycle from the following four aspects: (1) Model: NL2SQL translation techniques that tackle not only NL ambiguity and under-specification, but also properly map NL with database schema and instances; (2) Data: From the collection of training data, data synthesis due to training data scarcity, to NL2SQL benchmarks; (3) Evaluation: Evaluating NL2SQL methods from multiple angles using different metrics and granularities; and (4) Error Analysis: analyzing NL2SQL errors to find the root cause and guiding NL2SQL models to evolve. Moreover, we provide a rule of thumb for developing NL2SQL solutions. Finally, we discuss the research challenges and open problems of NL2SQL in the LLMs era.},
	language = {en-US},
	urldate = {2024-09-04},
	publisher = {arXiv},
	author = {Liu, Xinyu and Shen, Shuyu and Li, Boyan and Ma, Peixian and Jiang, Runzhi and Luo, Yuyu and Zhang, Yuxin and Fan, Ju and Li, Guoliang and Tang, Nan},
	month = aug,
	year = {2024},
	note = {arXiv:2408.05109 [cs]
TLDR: A comprehensive review of NL2SQL techniques powered by LLMs, covering its entire lifecycle from the following four aspects: model, data, evaluation, research challenges and open problems of NL2SQL in the LLMs era, and a rule of thumb for developing NL2SQL solutions.},
	keywords = {Computer Science - Databases},
	file = {arXiv Fulltext PDF:/Users/tangzhihao/Zotero/storage/RJZJHCXX/Liu 等 - 2024 - A Survey of NL2SQL with Large Language Models Where are we, and where are we going.pdf:application/pdf;arXiv.org Snapshot:/Users/tangzhihao/Zotero/storage/43RPL8N6/2408.html:text/html},
}


@misc{pourreza_dts-sql_2024,
	title = {{DTS}-{SQL}: {Decomposed} {Text}-to-{SQL} with {Small} {Large} {Language} {Models}},
	shorttitle = {{DTS}-{SQL}},
	url = {http://arxiv.org/abs/2402.01117},
	doi = {10.48550/arXiv.2402.01117},
	abstract = {Leading models for the text-to-SQL task heavily rely on proprietary Large Language Models (LLMs), posing concerns over data privacy. Closing the performance gap between small open-source models and large proprietary models is crucial to mitigate this reliance. To this end, we introduce a novel two-stage fine-tuning approach that decomposes the task into two simpler tasks. Through comprehensive evaluation on two large cross-domain datasets and two small LLMs, we show that this approach improves execution accuracy by 3 to 7 percent, effectively aligning the performance of open-source models with their proprietary counterparts.},
	language = {en-US},
	urldate = {2024-03-21},
	publisher = {arXiv},
	author = {Pourreza, Mohammadreza and Rafiei, Davood},
	month = feb,
	year = {2024},
	note = {arXiv:2402.01117 [cs]
TLDR: A novel two-stage fine-tuning approach is introduced that decomposes the text-to-SQL task into two simpler tasks and improves execution accuracy by 3 to 7 percent, effectively aligning the performance of open-source models with their proprietary counterparts.},
	keywords = {Computer Science - Computation and Language, Computer Science - Databases, Computer Science - Human-Computer Interaction},
	file = {arXiv Fulltext PDF:/Users/tangzhihao/Zotero/storage/4Z7LYHWA/Pourreza和Rafiei - 2024 - DTS-SQL Decomposed Text-to-SQL with Small Large Language Models.pdf:application/pdf;arXiv.org Snapshot:/Users/tangzhihao/Zotero/storage/22LH2UT6/2402.html:text/html},
}


