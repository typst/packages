@book{è’‹æœ‰ç»ª1998,
  title={ä¸­å›½æ£®æ—ç¾¤è½åˆ†ç±»åŠå…¶ç¾¤è½å­¦ç‰¹å¾},
  author={è’‹æœ‰ç»ª and éƒ­æ³‰æ°´ and é©¬å¨Ÿ and others},
  year={1998},
  publisher={ç§‘å­¦å‡ºç‰ˆç¤¾},
  address={åŒ—äº¬},
  pages={11-12},
}

@inproceedings{ä¸­å›½åŠ›å­¦å­¦ä¼š1990,
  title={ç¬¬3å±Šå…¨å›½å®éªŒæµä½“åŠ›å­¦å­¦æœ¯ä¼šè®®è®ºæ–‡é›†},
  author={ä¸­å›½åŠ›å­¦å­¦ä¼š},
  year={1990},
  address={å¤©æ´¥},
  publisher={**å‡ºç‰ˆç¤¾},
  pages={20-24}
}

@techreport{WHO1970,
  title={World Health Organization. Factors Regulating the Immune Response: Report of WHO Scientific Group},
  year={1970},
  institution={WHO},
  address={Geneva}
}

@phdthesis{å¼ å¿—ç¥¥1998,
  author = {å¼ å¿—ç¥¥},
  title = {é—´æ–­åŠ¨åŠ›ç³»ç»Ÿçš„éšæœºæ‰°åŠ¨åŠå…¶åœ¨å®ˆæ’å¾‹æ–¹ç¨‹ä¸­çš„åº”ç”¨},
  school = {åŒ—äº¬å¤§å­¦æ•°å­¦å­¦é™¢},
  address = {åŒ—äº¬},
  year = {1998},
  pages = {50-55}
}

@patent{æ²³åŒ—ç»¿æ´²2001,
  title={ä¸€ç§è’æ¼ åŒ–åœ°åŒºç”Ÿæ€æ¤è¢«ç»¼åˆåŸ¹è‚²ç§æ¤æ–¹æ³•:ä¸­å›½ï¼Œ01129210.5},
  author={æ²³åŒ—ç»¿æ´²ç”Ÿæ€ç¯å¢ƒç§‘æŠ€æœ‰é™å…¬å¸},
  year={2001},
  type={patent},
  number={01129210.5},
  note={2001-10-24[2002-05-28]},
  url={http://211.152.9.47/sipoasp/zlijs/hyjs-yxnew. asp?recid=01129210.5&leixin}
}

@standard{GB/T2659-1986,
  title = {ä¸–ç•Œå„å›½å’Œåœ°åŒºåç§°ä»£ç },
  author = {å›½å®¶æ ‡å‡†å±€ä¿¡æ¯åˆ†ç±»ç¼–ç ç ”ç©¶æ‰€},
  year = {1986},
  howpublished = {å…¨å›½æ–‡çŒ®å·¥ä½œæ ‡å‡†åŒ–æŠ€æœ¯å§”å‘˜ä¼š. æ–‡çŒ®å·¥ä½œå›½å®¶æ ‡å‡†æ±‡ç¼–:3.åŒ—äº¬:ä¸­å›½æ ‡å‡†å‡ºç‰ˆç¤¾ï¼Œ1988:59-92.},
  note = {GB/T 2659-1986}
}


@article{æç‚³ç©†2000,
  title={ç†æƒ³çš„å›¾ä¹¦é¦†å‘˜å’Œä¿¡æ¯ä¸“å®¶çš„ç´ è´¨ä¸å½¢è±¡},
  author={æç‚³ç©†},
  journal={å›¾ä¹¦æƒ…æŠ¥å·¥ä½œ},
  year={2000},
  volume={2000},
  number={2},
  pages={5-8}
}

@article{ä¸æ–‡ç¥¥2000,
  title={æ•°å­—é©å‘½ä¸ç«äº‰å›½é™…åŒ–},
  author={ä¸æ–‡ç¥¥},
  journal={ä¸­å›½é’å¹´æŠ¥},
  year={2000},
  month={11-20},
  number={15}
}

@article{æ±Ÿå‘ä¸œ1999,
  title={äº’è”ç½‘ç¯å¢ƒä¸‹çš„ä¿¡æ¯å¤„ç†ä¸å›¾ä¹¦ç®¡ç†ç³»ç»Ÿè§£å†³æ–¹æ¡ˆ},
  author={æ±Ÿå‘ä¸œ},
  journal={æƒ…æŠ¥å­¦æŠ¥},
  year={1999},
  volume={18},
  number={2},
  pages={4},
  note={2000-01-18},
  url={http://www.chinainfo.gov.cn/periodical/gbxb/gbxb99/gbxb990203}
}

@article{CHRISTINE1998,
  title={Plant physiology:plant biology in the Genome Era},
  author={M CHRISTINE},
  journal={Science},
  year={1998},
  volume={281},
  pages={331-332},
  note={1998-09-23},
  url={http://www.sciencemag.org/cgi/collection/anatmorp}
}


@inproceedings{zheng2022clrnet,
	address = {New Orleans, LA, USA},
	title = {{CLRNet}: {Cross} {Layer} {Refinement} {Network} for {Lane} {Detection}},
	isbn = {978-1-66546-946-3},
	shorttitle = {{CLRNet}},
	url = {https://ieeexplore.ieee.org/document/9879280/},
	doi = {10.1109/CVPR52688.2022.00097},
	abstract = {Lane is critical in the vision navigation system of the intelligent vehicle. Naturally, lane is a traffic sign with high-level semantics, whereas it owns the specific local pattern which needs detailed low-level features to localize accurately. Using different feature levels is of great importance for accurate lane detection, but it is still underexplored. In this work, we present Cross Layer Refinement Network (CLRNet) aiming at fully utilizing both highlevel and low-level features in lane detection. In particular, it first detects lanes with high-level semantic features then performs refinement based on low-level features. In this way, we can exploit more contextual information to detect lanes while leveraging local detailed lane features to improve localization accuracy. We present ROIGather to gather global context, which further enhances the feature representation of lanes. In addition to our novel network design, we introduce Line IoU loss which regresses the lane line as a whole unit to improve the localization accuracy. Experiments demonstrate that the proposed method greatly outperforms the state-of-the-art lane detection approaches.},
	language = {en},
	urldate = {2022-11-14},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Zheng, Tu and Huang, Yifei and Liu, Yang and Tang, Wenjian and Yang, Zheng and Cai, Deng and He, Xiaofei},
	month = jun,
	year = {2022},
	note = {TLDR: This work presents Cross Layer Refinement Network (CLRNet), a novel network design which first detects lanes with high-level semantic features then performs refinement based on low-level features and introduces Line IoU loss which regresses the lane line as a whole unit to improve the localization accuracy.},
	keywords = {/unread, ğŸ“’, â›” No INSPIRE recid found},
	pages = {888--897},
	file = {Zheng ç­‰ - 2022 - CLRNet Cross Layer Refinement Network for Lane De.pdf:D\:\\bw_ch\\Zotero\\storage\\PMLECK6C\\Zheng ç­‰ - 2022 - CLRNet Cross Layer Refinement Network for Lane De.pdf:application/pdf},
}

@article{liPETDetProposalEnhancement2023,
	title = {{PETDet}: {Proposal} {Enhancement} for {Two}-{Stage} {Fine}-{Grained} {Object} {Detection}},
	volume = {62},
	issn = {0196-2892, 1558-0644},
	shorttitle = {{PETDet}},
	url = {http://arxiv.org/abs/2312.10515},
	doi = {10/gtd5pk},
	abstract = {Fine-grained object detection (FGOD) extends object detection with the capability of fine-grained recognition. In recent two-stage FGOD methods, the region proposal serves as a crucial link between detection and fine-grained recognition. However, current methods overlook that some proposal-related procedures inherited from general detection are not equally suitable for FGOD, limiting the multi-task learning from generation, representation, to utilization. In this paper, we present PETDet (Proposal Enhancement for Two-stage fine-grained object detection) to better handle the sub-tasks in two-stage FGOD methods. Firstly, an anchor-free Quality Oriented Proposal Network (QOPN) is proposed with dynamic label assignment and attention-based decomposition to generate high-quality oriented proposals. Additionally, we present a Bilinear Channel Fusion Network (BCFN) to extract independent and discriminative features of the proposals. Furthermore, we design a novel Adaptive Recognition Loss (ARL) which offers guidance for the R-CNN head to focus on high-quality proposals. Extensive experiments validate the effectiveness of PETDet. Quantitative analysis reveals that PETDet with ResNet50 reaches state-of-the-art performance on various FGOD datasets, including FAIR1M-v1.0 (42.96 AP), FAIR1M-v2.0 (48.81 AP), MAR20 (85.91 AP) and ShipRSImageNet (74.90 AP). The proposed method also achieves superior compatibility between accuracy and inference speed. Our code and models will be released at https://github.com/canoe-Z/PETDet.},
	language = {en},
	urldate = {2023-12-27},
	journal = {IEEE Transactions on Geoscience and Remote Sensing},
	author = {Li, Wentao and Zhao, Danpei and Yuan, Bo and Gao, Yue and Shi, Zhenwei},
	year = {2023},
	note = {arXiv:2312.10515 [cs]
JCRåˆ†åŒº: Q1
ä¸­ç§‘é™¢åˆ†åŒºå‡çº§ç‰ˆ: åœ°çƒç§‘å­¦1åŒº
å½±å“å› å­: 7.5
5å¹´å½±å“å› å­: 7.6
EI: æ˜¯
CCF: B
ä¸­å›½åœ°è´¨å¤§å­¦: å·¥ç¨‹æŠ€æœ¯T2},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, /unread},
	pages = {1--1},
	file = {arXiv.org Snapshot:D\:\\bw_ch\\Zotero\\storage\\TVWQD4BH\\2312.html:text/html;arXiv.org Snapshot:D\:\\bw_ch\\Zotero\\storage\\G7DUHD6V\\2312.html:text/html;Li et al_2023_PETDet.pdf:D\:\\bw_ch\\Zotero\\storage\\V59WQC5V\\Li et al_2023_PETDet.pdf:application/pdf},
}

@inproceedings{wang2022keypointbased,
	address = {New Orleans, LA, USA},
	title = {A {Keypoint}-based {Global} {Association} {Network} for {Lane} {Detection}},
	isbn = {978-1-66546-946-3},
	url = {https://ieeexplore.ieee.org/document/9879757/},
	doi = {10.1109/CVPR52688.2022.00145},
	abstract = {Lane detection is a challenging task that requires predicting complex topology shapes of lane lines and distinguishing different types of lanes simultaneously. Earlier works follow a top-down roadmap to regress predefined anchors into various shapes of lane lines, which lacks enough flexibility to fit complex shapes of lanes due to the fixed anchor shapes. Lately, some works propose to formulate lane detection as a keypoint estimation problem to describe the shapes of lane lines more flexibly and gradually group adjacent keypoints belonging to the same lane line in a pointby-point manner, which is inefficient and time-consuming during postprocessing. In this paper, we propose a Global Association Network (GANet) to formulate the lane detection problem from a new perspective, where each keypoint is directly regressed to the starting point of the lane line instead of point-by-point extension. Concretely, the association of keypoints to their belonged lane line is conducted by predicting their offsets to the corresponding starting points of lanes globally without dependence on each other, which could be done in parallel to greatly improve efficiency. In addition, we further propose a Lane-aware Feature Aggregator (LFA), which adaptively captures the local correlations between adjacent keypoints to supplement local information to the global association. Extensive experiments on two popular lane detection benchmarks show that our method outperforms previous methods with F1 score of 79.63\% on CULane and 97.71\% on Tusimple dataset with high FPS. The code will be released at https://github.com/Wolfwjs/GANet.},
	language = {en},
	urldate = {2022-11-14},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Wang, Jinsheng and Ma, Yinchao and Huang, Shaofei and Hui, Tianrui and Wang, Fei and Qian, Chen and Zhang, Tianzhu},
	month = jun,
	year = {2022},
	note = {TLDR: A Global Association Network (GANet) is proposed to formulate the lane detection problem from a new perspective, where each keypoint is directly regressed to the starting point of the lane line instead of point-by-point extension, which outperforms previous methods.},
	keywords = {/unread},
	pages = {1382--1391},
	file = {Wang ç­‰ - 2022 - A Keypoint-based Global Association Network for La.pdf:D\:\\bw_ch\\Zotero\\storage\\8G7EDWDV\\Wang ç­‰ - 2022 - A Keypoint-based Global Association Network for La.pdf:application/pdf},
}

@inproceedings{lin2017feature,
	address = {Honolulu, HI},
	title = {Feature {Pyramid} {Networks} for {Object} {Detection}},
	isbn = {978-1-5386-0457-1},
	url = {http://ieeexplore.ieee.org/document/8099589/},
	doi = {10.1109/CVPR.2017.106},
	abstract = {Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But recent deep learning object detectors have avoided pyramid representations, in part because they are compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A topdown architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network (FPN), shows signiï¬cant improvement as a generic feature extractor in several applications. Using FPN in a basic Faster R-CNN system, our method achieves state-of-the-art singlemodel results on the COCO detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the COCO 2016 challenge winners. In addition, our method can run at 5 FPS on a GPU and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available.},
	language = {en},
	urldate = {2024-03-07},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Lin, Tsung-Yi and Dollar, Piotr and Girshick, Ross and He, Kaiming and Hariharan, Bharath and Belongie, Serge},
	month = jul,
	year = {2017},
	keywords = {/unread},
	pages = {936--944},
	file = {Lin ç­‰ - 2017 - Feature Pyramid Networks for Object Detection.pdf:D\:\\bw_ch\\Zotero\\storage\\KPWAITKE\\Lin ç­‰ - 2017 - Feature Pyramid Networks for Object Detection.pdf:application/pdf},
}

@inproceedings{zhangVIL100NewDataset2021,
	title = {{VIL}-100: {A} {New} {Dataset} and {A} {Baseline} {Model} for {Video} {Instance} {Lane} {Detection}},
	shorttitle = {{VIL}-100},
	url = {https://ieeexplore.ieee.org/document/9710077},
	doi = {10/gp8m7x},
	abstract = {Lane detection plays a key role in autonomous driving. While car cameras always take streaming videos on the way, current lane detection works mainly focus on individual images (frames) by ignoring dynamics along the video. In this work, we collect a new video instance lane detection (VIL-100) dataset, which contains 100 videos with in total 10,000 frames, acquired from different real traffic scenarios. All the frames in each video are manually annotated to a high-quality instance-level lane annotation, and a set of frame-level and video-level metrics are included for quantitative performance evaluation. Moreover, we propose a new baseline model, named multi-level memory aggregation network (MMA-Net), for video instance lane detection. In our approach, the representation of current frame is enhanced by attentively aggregating both local and global memory features from other frames. Experiments on the new collected dataset show that the proposed MMA-Net outperforms state-of-the-art lane detection methods and video object segmentation methods. We release our dataset and code at https://github.com/yujun0-0/MMA-Net.},
	language = {en},
	urldate = {2024-01-17},
	booktitle = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Zhang, Yujun and Zhu, Lei and Feng, Wei and Fu, Huazhu and Wang, Mingqian and Li, Qingxia and Li, Cheng and Wang, Song},
	month = oct,
	year = {2021},
	note = {ISSN: 2380-7504},
	keywords = {/unread},
	pages = {15661--15670},
	file = {IEEE Xplore Abstract Record:D\:\\bw_ch\\Zotero\\storage\\35I4UD44\\9710077.html:text/html;Zhang et al_2021_VIL-100.pdf:D\:\\bw_ch\\Zotero\\storage\\DG25B5IW\\Zhang et al_2021_VIL-100.pdf:application/pdf},
}

@article{panSpatialDeepSpatial2018,
	title = {Spatial as {Deep}: {Spatial} {CNN} for {Traffic} {Scene} {Understanding}},
	volume = {32},
	copyright = {Copyright (c)},
	issn = {2374-3468},
	shorttitle = {Spatial as {Deep}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/12301},
	doi = {10/gqzs6n},
	abstract = {Convolutional neural networks (CNNs) are usually built by stacking convolutional operations layer-by-layer. Although CNN has shown strong capability to extract semantics from raw pixels, its capacity to capture spatial relationships of pixels across rows and columns of an image is not fully explored. These relationships are important to learn semantic objects with strong shape priors but weak appearance coherences, such as traffic lanes, which are often occluded or not even painted on the road surface as shown in Fig. 1 (a). In this paper, we propose Spatial CNN (SCNN), which generalizes traditional deep layer-by-layer convolutions to slice-by-slice convolutions within feature maps, thus enabling message passings between pixels across rows and columns in a layer. Such SCNN is particular suitable for long continuous shape structure or large objects, with strong spatial relationship but less appearance clues, such as traffic lanes, poles, and wall. We apply SCNN on a newly released very challenging traffic lane detection dataset and Cityscapse dataset. The results show that SCNN could learn the spatial relationship for structure output and significantly improves the performance. We show that SCNN outperforms the recurrent neural network (RNN) based ReNet and MRF+CNN (MRFNet) in the lane detection dataset by 8.7\% and 4.6\% respectively. Moreover, our SCNN won the 1st place on the TuSimple Benchmark Lane Detection Challenge, with an accuracy of 96.53\%.},
	language = {en},
	number = {1},
	urldate = {2024-01-17},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Pan, Xingang and Shi, Jianping and Luo, Ping and Wang, Xiaogang and Tang, Xiaoou},
	month = apr,
	year = {2018},
	note = {Number: 1},
	keywords = {/unread, Spatial CNN},
	file = {Pan et al_2018_Spatial as Deep.pdf:D\:\\bw_ch\\Zotero\\storage\\66N5BXUL\\Pan et al_2018_Spatial as Deep.pdf:application/pdf},
}

@article{liLineCNNEndEndTraffic2020,
	title = {Line-{CNN}: {End}-to-{End} {Traffic} {Line} {Detection} {With} {Line} {Proposal} {Unit}},
	volume = {21},
	issn = {1558-0016},
	shorttitle = {Line-{CNN}},
	url = {https://ieeexplore.ieee.org/document/8624563},
	doi = {10/ghfvqj},
	abstract = {The task of traffic line detection is a fundamental yet challenging problem. Previous approaches usually conduct traffic line detection via a two-stage way, namely the line segment detection followed by a segment clustering, which is very likely to ignore the global semantic information of an entire line. To address the problem, we propose an end-to-end system called Line-CNN (L-CNN), in which the key component is a novel line proposal unit (LPU). The LPU utilizes line proposals as references to locate accurate traffic curves, which forces the system to learn the global feature representation of the entire traffic lines. We benchmark the proposed L-CNN on two public datasets including MIKKI and TuSimple, and the results suggest that L-CNN outperforms the state-of-the-art methods. In addition, L-CNN can run at approximately 30 f/s on a Titan X GPU, which indicates the practicability and effectiveness of L-CNN for real-time intelligent self-driving systems.},
	language = {en},
	number = {1},
	urldate = {2024-01-21},
	journal = {IEEE Transactions on Intelligent Transportation Systems},
	author = {Li, Xiang and Li, Jun and Hu, Xiaolin and Yang, Jian},
	month = jan,
	year = {2020},
	note = {Conference Name: IEEE Transactions on Intelligent Transportation Systems},
	keywords = {/unread},
	pages = {248--258},
	file = {IEEE Xplore Abstract Record:D\:\\bw_ch\\Zotero\\storage\\V48TJ2NM\\8624563.html:text/html;Li et al_2020_Line-CNN.pdf:D\:\\bw_ch\\Zotero\\storage\\YHP3KWQZ\\Li et al_2020_Line-CNN.pdf:application/pdf;Li et al_2020_Line-CNN.pdf:D\:\\bw_ch\\Zotero\\storage\\KEMFMEQD\\Li et al_2020_Line-CNN.pdf:application/pdf},
}

@article{tabeliniLaneMarkingDetection2022,
	title = {Lane {Marking} {Detection} and {Classification} using {Spatial}-{Temporal} {Feature} {Pooling}},
	url = {https://ieeexplore.ieee.org/document/9892478},
	doi = {10.1109/IJCNN55064.2022.9892478},
	abstract = {The lane detection problem has been extensively researched in the past decades, especially since the advent of deep learning. Despite the numerous works proposing solutions to the localization task (i.e., localizing the lane boundaries in an input image), the classification task has not seen the same focus. Nonetheless, knowing the type of lane boundary, particularly that of the ego lane, can be very useful for many applications. For instance, a vehicle might not be allowed by law to overtake depending on the type of the ego lane. Beyond that, very few works take advantage of the temporal information available in the videos captured by the vehicles: most methods employ a single-frame approach. In this work, building upon the recent LaneATT model, we propose an approach to exploit the temporal information and integrate the classification task into the model. Our results show that the proposed modifications can improve the detection performance on the most recent benchmark by 2.34 \%, establishing a new state-of-the-art. Finally, an extensive evaluation shows that it enables a high classification performance (89.37 \%) that serves as a future benchmark for the field.},
	language = {en},
	urldate = {2024-05-22},
	journal = {2022 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {Tabelini, Lucas and Berriel, Rodrigo and De Souza, Alberto F. and Badue, Claudine and Oliveira-Santos, Thiago},
	month = jul,
	year = {2022},
	note = {ISSN: 2161-4407},
	keywords = {/unread, Deep learning, deep learning, Benchmark testing, lane classification, lane detection, Lane detection, Location awareness, Neural networks, Predictive models, temporal information, Transforms},
	pages = {1--7},
	file = {IEEE Xplore Abstract Record:D\:\\bw_ch\\Zotero\\storage\\FMVBQI5W\\9892478.html:text/html;Tabelini et al_2022_Lane Marking Detection and Classification using Spatial-Temporal Feature Pooling.pdf:D\:\\bw_ch\\Zotero\\storage\\TVK3D2JU\\Tabelini et al_2022_Lane Marking Detection and Classification using Spatial-Temporal Feature Pooling.pdf:application/pdf},
}

@article{zhangLaneDetectionModel2022,
	title = {Lane {Detection} {Model} {Based} on {Spatio}-{Temporal} {Network} {With} {Double} {Convolutional} {Gated} {Recurrent} {Units}},
	volume = {23},
	issn = {1558-0016},
	url = {https://ieeexplore.ieee.org/document/9364924},
	doi = {10.1109/TITS.2021.3060258},
	abstract = {Lane detection is one of the indispensable and key elements of self-driving environmental perception. Many lane detection models have been proposed, solving lane detection under challenging conditions, including intersection merging and splitting, curves, boundaries, occlusions and combinations of scene types. Nevertheless, lane detection will remain an open problem for some time to come. The ability to cope well with those challenging scenes impacts greatly the applications of lane detection on advanced driver assistance systems (ADASs). In this paper, a spatio-temporal network with double Convolutional Gated Recurrent Units (ConvGRUs) is proposed to address lane detection in challenging scenes. Both of ConvGRUs have the same structures, but different locations and functions in our network. One is used to extract the information of the most likely low-level features of lane markings. The extracted features are input into the next layer of the end-to-end network after concatenating them with the outputs of some blocks. The other one takes some continuous frames as its input to process the spatio-temporal driving information. Extensive experiments on the large-scale TuSimple lane marking challenge dataset and Unsupervised LLAMAS dataset demonstrate that the proposed model can effectively detect lanes in the challenging driving scenes. Our model can outperform the state-of-the-art lane detection models.},
	language = {en},
	number = {7},
	urldate = {2024-05-18},
	journal = {IEEE Transactions on Intelligent Transportation Systems},
	author = {Zhang, Jiyong and Deng, Tao and Yan, Fei and Liu, Wenbo},
	month = jul,
	year = {2022},
	note = {Conference Name: IEEE Transactions on Intelligent Transportation Systems
TLDR: A spatio-temporal network with double Convolutional Gated Recurrent Units (ConvGRUs) is proposed to address lane detection in challenging scenes and can outperform the state-of-the-art lane detection models.},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, /unread, Deep learning, Feature extraction, Lane detection, ConvGRUs, convolutional neural network, end-to-end, Image segmentation, Logic gates, Roads, Semantics, spatio-temporal},
	pages = {6666--6678},
	file = {arXiv.org Snapshot:D\:\\bw_ch\\Zotero\\storage\\3ENFZAMD\\2008.html:text/html;Zhang et al_2022_Lane Detection Model Based on Spatio-Temporal Network With Double Convolutional.pdf:D\:\\bw_ch\\Zotero\\storage\\ICIYJGFV\\Zhang et al_2022_Lane Detection Model Based on Spatio-Temporal Network With Double Convolutional.pdf:application/pdf},
}

@article{zouRobustLaneDetection2020,
	title = {Robust {Lane} {Detection} from {Continuous} {Driving} {Scenes} {Using} {Deep} {Neural} {Networks}},
	volume = {69},
	issn = {0018-9545, 1939-9359},
	url = {http://arxiv.org/abs/1903.02193},
	doi = {10.1109/TVT.2019.2949603},
	abstract = {Lane detection in driving scenes is an important module for autonomous vehicles and advanced driver assistance systems. In recent years, many sophisticated lane detection methods have been proposed. However, most methods focus on detecting the lane from one single image, and often lead to unsatisfactory performance in handling some extremely-bad situations such as heavy shadow, severe mark degradation, serious vehicle occlusion, and so on. In fact, lanes are continuous line structures on the road. Consequently, the lane that cannot be accurately detected in one current frame may potentially be inferred out by incorporating information of previous frames. To this end, we investigate lane detection by using multiple frames of a continuous driving scene, and propose a hybrid deep architecture by combining the convolutional neural network (CNN) and the recurrent neural network (RNN). Specifically, information of each frame is abstracted by a CNN block, and the CNN features of multiple continuous frames, holding the property of time-series, are then fed into the RNN block for feature learning and lane prediction. Extensive experiments on two large-scale datasets demonstrate that, the proposed method outperforms the competing methods in lane detection, especially in handling difficult situations.},
	language = {en},
	number = {1},
	urldate = {2024-05-18},
	journal = {IEEE Transactions on Vehicular Technology},
	author = {Zou, Qin and Jiang, Hanwen and Dai, Qiyu and Yue, Yuanhao and Chen, Long and Wang, Qian},
	month = jan,
	year = {2020},
	note = {arXiv:1903.02193 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, /unread},
	pages = {41--54},
	file = {arXiv.org Snapshot:D\:\\bw_ch\\Zotero\\storage\\WPNTN9MA\\1903.html:text/html;Zou et al_2020_Robust Lane Detection from Continuous Driving Scenes Using Deep Neural Networks.pdf:D\:\\bw_ch\\Zotero\\storage\\XCPGMC2Q\\Zou et al_2020_Robust Lane Detection from Continuous Driving Scenes Using Deep Neural Networks.pdf:application/pdf},
}

@inproceedings{dai2021attentional,
	title = {Attentional {Feature} {Fusion}},
	url = {https://openaccess.thecvf.com/content/WACV2021/html/Dai_Attentional_Feature_Fusion_WACV_2021_paper.html},
	language = {en},
	urldate = {2024-02-29},
	booktitle = {{IEEE}/{CVF} {Winter} {Conference} on {Applications} of {Computer} {Vision} ({WACV})},
	author = {Dai, Yimian and Gieseke, Fabian and Oehmcke, Stefan and Wu, Yiquan and Barnard, Kobus},
	year = {2021},
	keywords = {/unread},
	pages = {3560--3569},
	file = {Dai et al_2021_Attentional Feature Fusion.pdf:D\:\\bw_ch\\Zotero\\storage\\GGL4K4Q8\\Dai et al_2021_Attentional Feature Fusion.pdf:application/pdf},
}

@article{liang2020lane,
	title = {Lane {Detection}: a {Survey} with {New} {Results}},
	volume = {35},
	language = {en},
	number = {3},
	journal = {Journal of Computer Science and Technology},
	author = {Liang, Dun and Guo, Yuan-Chen and Zhang, Shao-Kui and Mu, Tai-Jiang and Huang, Xiaolei},
	year = {2020},
	note = {Publisher: Springer Science and Business Media LLC},
	keywords = {/unread},
	pages = {493--505},
}

@article{YinZhangCaiZiDongJiaShiGaoJingDiTuDeXinXiChuanShuMoXing2024,
	title = {è‡ªåŠ¨é©¾é©¶é«˜ç²¾åœ°å›¾çš„ä¿¡æ¯ä¼ è¾“æ¨¡å‹},
	volume = {49},
	issn = {1671-8860},
	url = {https://kns.cnki.net/kcms2/article/abstract?v=axnrJTP8flwIIog7bfPjWLX9aiyRtZbrG_-hCEGoEIqm_CFj25YOCJF2F_aE4lhbNF0LKF0pkdIZ99jB-BL-E5IRL-a3uee5JFMJ2CI_Mv5ikwYrmLv_G8YG2aIGMsqWkrVSOpzKO7lVTfvnyE_Gcw==&uniplatform=NZKPT&language=CHS},
	doi = {10.13203/j.whugis20230135},
	abstract = {é«˜ç²¾åœ°å›¾çš„â€œéè§†è§‰â€å’Œé¢å‘æœºå™¨ç‰¹æ€§ï¼Œä½¿å…¶ä¸ä¼ ç»Ÿé¢å‘äººçš„æ—¶ç©ºäº§å“æœ‰æ˜æ˜¾ä¸åŒï¼Œç›¸åº”çš„æè¿°åœ°å›¾ä¸»ã€å®¢ä½“åŠå…¶ä¸äº§å“ä¹‹é—´å…³ç³»çš„ä¼ è¾“æ¨¡å‹ä¹Ÿé¢ä¸´å·¨å¤§å˜é©ã€‚å·²æœ‰çš„ä¼ è¾“æ¨¡å‹é‡æ„äº†ä¸Šè¿°å…³ç³»ï¼ŒåŒ…æ‹¬æ–°å¢ç”¨æˆ·ä¸ªæ€§ä¿¡æ¯åŠå…¶ä¼ è¾“ï¼Œä½†ä¹Ÿå­˜åœ¨ä¿¡æ¯ä¼ é€’å·¥å…·ä»é‡‡ç”¨é¢å‘äººè€Œéæœºå™¨çš„åœ°å›¾è¯­è¨€ç­‰ä¸è¶³ã€‚ä¸ºæ­¤ï¼Œç»“åˆè‡ªåŠ¨é©¾é©¶ä¸­åœ°å›¾ä¿¡æ¯çš„ä¼ è¾“ç‰¹ç‚¹ï¼Œæ„å»ºé¢å‘æœºå™¨è®¤çŸ¥çš„é«˜ç²¾åœ°å›¾ä¿¡æ¯ä¼ è¾“æ¨¡å‹ï¼Œé€šè¿‡å¯¹å·²æœ‰çš„ä¼ è¾“æ¨¡å‹è¿›è¡Œæ‰©å±•ï¼šGISè¯­è¨€ä»£æ›¿åœ°å›¾è¯­è¨€ï¼Œå°†ç”¨æˆ·ä¸ªæ€§ä¿¡æ¯æ•´åˆåˆ°é«˜ç²¾åœ°å›¾çš„ç”¨æˆ·å›¾å±‚ä¸­ï¼Œå°†è¡ŒåŠ¨æŒ‡å¯¼æ‰©å±•ä¸ºè¡ŒåŠ¨å®è·µã€‚ç ”ç©¶ç»“æœè¡¨æ˜,æ„å»ºçš„å…¨æœºå™¨è®¤çŸ¥çš„é«˜ç²¾åœ°å›¾ä¿¡æ¯ä¼ è¾“æ¨¡å‹å®ç°äº†åœ°å›¾ä¿¡æ¯è®¤çŸ¥çš„ä¸»ä½“ç”±äººåˆ°æœºå™¨çš„æ‰©å±•ï¼Œé€‚åº”äº†é«˜ç²¾åœ°å›¾åœ¨æ„ŸçŸ¥ã€å®šä½ã€è§„åˆ’ã€æ§åˆ¶ç­‰ä¼ è¾“è¿‡ç¨‹ä¸­å…¨äººå·¥æ™ºèƒ½çš„ç‰¹æ€§ã€‚æ‰€æå‡ºçš„æ¨¡å‹ä¸€æ–¹é¢æœ‰åŠ©äºå‡†ç¡®æŠŠæ¡é«˜ç²¾åœ°å›¾çš„æœ¬è´¨åŠå†…å®¹ç»“æ„ï¼Œæå‡è®¤çŸ¥æ•ˆæœï¼›å¦ä¸€æ–¹é¢ï¼Œå¯¹é©¾é©¶æœåŠ¡å…·æœ‰é‡è¦çš„æŒ‡å¯¼ä½œç”¨ï¼ŒåŒ…æ‹¬å†…å®¹é€‰å–ã€è¡¨è¾¾æ–¹æ³•ã€ç³»ç»ŸåŠŸèƒ½æ¡†æ¶è®¾è®¡ç­‰ï¼Œæé«˜ä¼ è¾“æ•ˆç‡ã€‚},
	language = {zh},
	number = {4},
	urldate = {2024-05-22},
	journal = {æ­¦æ±‰å¤§å­¦å­¦æŠ¥(ä¿¡æ¯ç§‘å­¦ç‰ˆ)},
	author = {{å°¹ç« æ‰} and {é½å¦‚ç…œ} and {åº”ç”³}},
	year = {2024},
	keywords = {/unread, åœ°å›¾ç©ºé—´è®¤çŸ¥, åœ°å›¾ä¿¡æ¯ä¼ è¾“æ¨¡å‹, é«˜ç²¾åœ°å›¾, è‡ªåŠ¨é©¾é©¶},
	pages = {527--536},
}

@inproceedings{cai2018cascade,
	title = {Cascade {R}-{CNN}: {Delving} {Into} {High} {Quality} {Object} {Detection}.},
	language = {en},
	booktitle = {Computer {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Cai, Zhaowei and Vasconcelos, Nuno},
	year = {2018},
	keywords = {/unread},
	pages = {6154--6162},
}

@article{ZangJinHuanXinNengYuanQiCheChanYeFaZhanGuiHua20212035NianDiaoZhengJieDu2021,
	title = {ã€Šæ–°èƒ½æºæ±½è½¦äº§ä¸šå‘å±•è§„åˆ’(2021â€”2035å¹´)ã€‹è°ƒæ•´è§£è¯»},
	issn = {2095-9044},
	url = {https://kns.cnki.net/kcms2/article/abstract?v=axnrJTP8flzVDV_4zpnHJrtTvp0I62PSoywJYXyolsK6san85xDaTLNyGCiLlybAgiaur5yQcox4NCVX_O4Ed8KAl1ozIf5sRnoD4mMVsn4MId8S0roZczzm3JLTc_ZfYGRo9ZbsIW8mYya8c07b2A==&uniplatform=NZKPT&language=CHS},
	doi = {10.16173/j.cnki.ame.2021.z1.019},
	abstract = {{\textless}æ­£{\textgreater}2020å¹´11æœˆ2æ—¥,å›½åŠ¡é™¢åŠå…¬å…å‘å¸ƒã€Šæ–°èƒ½æºæ±½è½¦äº§ä¸šå‘å±•è§„åˆ’(2021â€”2035å¹´)ã€‹(ä»¥ä¸‹ç®€ç§°ã€Šè§„åˆ’ã€‹),è¿™æ˜¯ç»§ã€ŠèŠ‚èƒ½ä¸æ–°èƒ½æºæ±½è½¦äº§ä¸šå‘å±•è§„åˆ’(2012â€”2020å¹´)ã€‹å,æˆ‘å›½å…³äºæ–°èƒ½æºæ±½è½¦äº§ä¸šçš„åˆä¸€çº²é¢†æ€§æ–‡ä»¶,æ˜ç¡®äº†ä¸­å›½æ–°èƒ½æºæ±½è½¦å‘å±•çš„æ„¿æ™¯å’Œç›¸åº”éƒ¨ç½²ã€‚å€¼å¾—ä¸€æçš„æ˜¯,ç”±äºæ²¹è€—é—®é¢˜å·²ç»æœ‰ç›¸åº”çš„æ ‡å‡†å’Œè¦æ±‚,æ‰€ä»¥æ­¤æ¬¡ã€Šè§„åˆ’ã€‹å¹¶æœªæ¶‰åŠä¼ ç»Ÿç‡ƒæ²¹æ±½è½¦çš„èŠ‚èƒ½é—®é¢˜,è€Œæ˜¯æŠŠé‡ç‚¹èšç„¦åˆ°æ–°èƒ½æºæ±½è½¦äº§ä¸šçš„å‘å±•ã€‚},
	language = {zh},
	number = {Z1},
	urldate = {2024-05-22},
	journal = {æ±½è½¦å·¥è‰ºå¸ˆ},
	author = {{è‡§é‡‘ç¯} and {ææ˜¥ç²}},
	year = {2021},
	keywords = {/unread, ã€Šæ–°èƒ½æºæ±½è½¦äº§ä¸šå‘å±•è§„åˆ’(2021â€”2035å¹´)ã€‹, å……ç”µåŸºç¡€è®¾æ–½, æ°¢ç‡ƒæ–™æ±½è½¦, æ–°èƒ½æºæ±½è½¦äº§ä¸š, ä¿¡æ¯é€šä¿¡èåˆ, æ™ºèƒ½ç½‘è”æ±½è½¦},
	pages = {32--34},
	file = {è‡§é‡‘ç¯_ææ˜¥ç²_2021_ã€Šæ–°èƒ½æºæ±½è½¦äº§ä¸šå‘å±•è§„åˆ’(2021â€”2035å¹´)ã€‹è°ƒæ•´è§£è¯».pdf:D\:\\bw_ch\\Zotero\\storage\\KVMWBQDJ\\è‡§é‡‘ç¯_ææ˜¥ç²_2021_ã€Šæ–°èƒ½æºæ±½è½¦äº§ä¸šå‘å±•è§„åˆ’(2021â€”2035å¹´)ã€‹è°ƒæ•´è§£è¯».pdf:application/pdf},
}

@inproceedings{feng2022rethinking,
	title = {Rethinking {Efficient} {Lane} {Detection} via {Curve} {Modeling}.},
	language = {en},
	booktitle = {Computer {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Feng, Zhengyang and Guo, Shaohua and Tan, Xin and Xu, Ke and Wang, Min and Ma, Lizhuang},
	year = {2022},
	keywords = {/unread},
	pages = {17041--17049},
}

@article{jeongTutorialHighDefinitionMap2022,
	title = {Tutorial on {High}-{Definition} {Map} {Generation} for {Automated} {Driving} in {Urban} {Environments}},
	volume = {22},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/22/18/7056},
	doi = {10.3390/s22187056},
	abstract = {High-definition (HD) mapping is a promising approach to realize highly automated driving (AD). Although HD maps can be applied to all levels of autonomy, their use is particularly beneficial for autonomy levels 4 or higher. HD maps enable AD systems to see beyond the field of view of conventional sensors, thereby providing accurate and detailed information regarding a driving environment. An HD map is typically separated into a pointcloud map for localization and a vector map for path planning. In this paper, we introduce two separate but successive HD map generation workflows. Of the several stages involved, the registration and mapping processes are essential for creating the pointcloud and vector maps, respectively. To facilitate the readersâ€™ understanding, the processes of these two stages have been recorded and uploaded online. HD maps are typically generated using open-source software (OSS) tools. CloudCompare and ASSURE, as representative tools, are used in this study. The generated HD maps are validated with localization and path-planning modules in Autoware, which is also an OSS stack for AD systems. The generated HD maps enable environmental-monitoring vehicles to successfully operate at level 4 autonomy.},
	language = {en},
	number = {18},
	urldate = {2024-05-22},
	journal = {Sensors},
	author = {Jeong, Jinseop and Yoon, Jun Yong and Lee, Hwanhong and Darweesh, Hatem and Sung, Woosuk},
	month = jan,
	year = {2022},
	note = {Number: 18
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {/unread, autonomous driving, high-definition map, localization, path planning},
	pages = {7056},
	file = {Jeong et al_2022_Tutorial on High-Definition Map Generation for Automated Driving in Urban.pdf:D\:\\bw_ch\\Zotero\\storage\\L3HWQHCK\\Jeong et al_2022_Tutorial on High-Definition Map Generation for Automated Driving in Urban.pdf:application/pdf},
}

@article{ebrahimisoorchaeiHighDefinitionMapRepresentation2022,
	title = {High-{Definition} {Map} {Representation} {Techniques} for {Automated} {Vehicles}},
	volume = {11},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2079-9292},
	url = {https://www.mdpi.com/2079-9292/11/20/3374},
	doi = {10.3390/electronics11203374},
	abstract = {Many studies in the field of robot navigation have focused on environment representation and localization. The goal of map representation is to summarize spatial information in topological and geometrical abstracts. By providing strong priors, maps improve the performance and reliability of automated robots. Due to the transition to fully automated driving in recent years, there has been a constant effort to design methods and technologies to improve the precision of road participants and the environmentâ€™s information. Among these efforts is the high-definition (HD) map concept. Making HD maps requires accuracy, completeness, verifiability, and extensibility. Because of the complexity of HD mapping, it is currently expensive and difficult to implement, particularly in an urban environment. In an urban traffic system, the road model is at least a map with sets of roads, lanes, and lane markers. While more research is being dedicated to mapping and localization, a comprehensive review of the various types of map representation is still required. This paper presents a brief overview of map representation, followed by a detailed literature review of HD maps for automated vehicles. The current state of autonomous vehicle (AV) mapping is encouraging, the field has matured to a point where detailed maps of complex environments are built in real time and have been proved useful. Many existing techniques are robust to noise and can cope with a large range of environments. Nevertheless, there are still open problems for future research. AV mapping will continue to be a highly active research area essential to the goal of achieving full autonomy.},
	language = {en},
	number = {20},
	urldate = {2024-05-22},
	journal = {Electronics},
	author = {Ebrahimi Soorchaei, Babak and Razzaghpour, Mahdi and Valiente, Rodolfo and Raftari, Arash and Fallah, Yaser Pourmohammadi},
	month = jan,
	year = {2022},
	note = {Number: 20
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {/unread, connected and automated vehicles, high-definition (HD) map, map representation, navigation},
	pages = {3374},
	file = {Ebrahimi Soorchaei et al_2022_High-Definition Map Representation Techniques for Automated Vehicles.pdf:D\:\\bw_ch\\Zotero\\storage\\75U7M9UE\\Ebrahimi Soorchaei et al_2022_High-Definition Map Representation Techniques for Automated Vehicles.pdf:application/pdf},
}

@inproceedings{jin2023recursive,
	title = {Recursive {Video} {Lane} {Detection}},
	language = {en},
	booktitle = {2023 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Jin, Dongkwon and Kim, Dahyun and Kim, Chang-Su},
	year = {2023},
	keywords = {/unread},
	pages = {8439--8448},
}

@article{ko2022points,
	title = {Key {Points} {Estimation} and {Point} {Instance} {Segmentation} {Approach} for {Lane} {Detection}.},
	volume = {23},
	language = {en},
	number = {7},
	journal = {IEEE Transactions on Intelligent Transportation Systems},
	author = {Ko, YeongMin and Lee, Younkwan and Azam, Shoaib and Munir, Farzeen and Jeon, Moongu and Pedrycz, Witold},
	year = {2022},
	keywords = {/unread},
	pages = {8949--8958},
}

@article{liuVisionbasedEnvironmentalPerception2023,
	title = {Vision-based environmental perception for autonomous driving},
	issn = {0954-4070},
	url = {https://doi.org/10.1177/09544070231203059},
	doi = {10.1177/09544070231203059},
	abstract = {Visual perception plays an important role in autonomous driving. One of the primary tasks is object detection and identification. Since the vision sensor is rich in color and texture information, it can quickly and accurately identify various road information. The commonly used technique is based on extracting and calculating various features of the image. The recent development of deep learning-based method has better reliability and processing speed and has a greater advantage in recognizing complex elements. For depth estimation, vision sensor is also used for ranging due to their small size and low cost. Monocular camera uses image data from a single viewpoint as input to estimate object depth. In contrast, stereo vision is based on parallax and matching feature points of different views, and the application of Deep learning also further improves the accuracy. In addition, Simultaneous Location and Mapping (SLAM) can establish a model of the road environment, thus helping the vehicle perceive the surrounding environment and complete the tasks. In this paper, we introduce and compare various methods of object detection and identification, then explain the development of depth estimation and compare various methods based on monocular, stereo, and RGB-D sensors, next review and compare various methods of SLAM, and finally summarize the current problems and present the future development trends of vision technologies.},
	language = {en},
	urldate = {2024-05-22},
	journal = {Proceedings of the Institution of Mechanical Engineers, Part D: Journal of Automobile Engineering},
	author = {Liu, Fei and Lu, Zihao and Lin, Xianke},
	month = nov,
	year = {2023},
	note = {Publisher: IMECHE},
	keywords = {/unread},
	pages = {09544070231203059},
	file = {Liu et al_2023_Vision-based environmental perception for autonomous driving.pdf:D\:\\bw_ch\\Zotero\\storage\\8YFKF7WW\\Liu et al_2023_Vision-based environmental perception for autonomous driving.pdf:application/pdf},
}

@inproceedings{liu2021endtoend,
	title = {End-to-end {Lane} {Shape} {Prediction} with {Transformers}.},
	language = {en},
	booktitle = {{IEEE}/{CVF} {Winter} {Conference} on {Applications} of {Computer} {Vision} ({WACV})},
	author = {Liu, Ruijin and Yuan, Zejian and Liu, Tie and Xiong, Zhiliang},
	year = {2021},
	keywords = {/unread},
	pages = {3693--3701},
}

@inproceedings{liu2021condlanenet,
	title = {{CondLaneNet}: a {Top}-to-down {Lane} {Detection} {Framework} {Based} on {Conditional} {Convolution}.},
	language = {en},
	booktitle = {{IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Liu, Lizhe and Chen, Xiaohao and Zhu, Siyu and Tan, Ping},
	year = {2021},
	keywords = {/unread},
	pages = {3753--3762},
}

@inproceedings{meng2021conditional,
	title = {Conditional {DETR} for {Fast} {Training} {Convergence}.},
	language = {en},
	booktitle = {{IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Meng, Depu and Chen, Xiaokang and Fan, Zejia and Zeng, Gang and Li, Houqiang and Yuan, Yuhui and Sun, Lei and Wang, Jingdong},
	year = {2021},
	keywords = {/unread},
	pages = {3631--3640},
}

@article{qin2024ultra,
	title = {Ultra {Fast} {Deep} {Lane} {Detection} with {Hybrid} {Anchor} {Driven} {Ordinal} {Classification}},
	language = {en},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Qin, Zequn and Zhang, Pengyi and Li, Xi},
	year = {2024},
	note = {Publisher: IEEE},
	keywords = {/unread},
	pages = {1--14},
}

@inproceedings{qu2021focus,
	title = {Focus on {Local}: {Detecting} {Lane} {Marker} {From} {Bottom} {Up} via {Key} {Point}.},
	language = {en},
	booktitle = {Computer {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Qu, Zhan and Jin, Huan and Zhou, Yang and Yang, Zhen and Zhang, Wei},
	year = {2021},
	keywords = {/unread},
	pages = {14122--14130},
}

@inproceedings{su2021structure,
	title = {Structure {Guided} {Lane} {Detection}.},
	language = {en},
	booktitle = {International {Joint} {Conference} on {Artificial} {Intelligence} ({IJCAI})},
	author = {Su, Jinming and Chen, Chao and Zhang, Ke and Luo, Junfeng and Wei, Xiaoming and Wei, Xiaolin},
	year = {2021},
	keywords = {/unread},
	pages = {997--1003},
}

@inproceedings{torres2020polylanenet,
	title = {{PolyLaneNet}: {Lane} {Estimation} via {Deep} {Polynomial} {Regression}.},
	language = {en},
	booktitle = {International {Conference} on {Pattern} {Recognition} ({ICPR})},
	author = {Torres, Lucas Tabelini and Berriel, Rodrigo Ferreira and PaixÃ£o, Thiago M. and Badue, Claudine and Souza, Alberto F. De and Oliveira-Santos, Thiago},
	year = {2020},
	keywords = {/unread},
	pages = {6150--6156},
}

@inproceedings{torres2021keep,
	title = {Keep {Your} {Eyes} on the {Lane}: {Real}-{Time} {Attention}-{Guided} {Lane} {Detection}.},
	language = {en},
	booktitle = {Computer {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Torres, Lucas Tabelini and Berriel, Rodrigo Ferreira and PaixÃ£o, Thiago M. and Badue, Claudine and Souza, Alberto F. De and Oliveira-Santos, Thiago},
	year = {2021},
	keywords = {/unread},
	pages = {294--302},
}

@inproceedings{wangVideoInstanceLane2022,
	address = {Lisboa Portugal},
	title = {Video {Instance} {Lane} {Detection} via {Deep} {Temporal} and {Geometry} {Consistency} {Constraints}},
	isbn = {978-1-4503-9203-7},
	url = {https://dl.acm.org/doi/10.1145/3503161.3547914},
	doi = {10.1145/3503161.3547914},
	abstract = {Video instance lane detection is one of the most important tasks in autonomous driving. Due to the very sparse region and weak context in lane annotations, accurately detecting instance-level lanes in real-world traffic scenarios is challenging, especially for scenes with occlusion, bad weather conditions, dim or dazzling lights. Current methods mainly address this problem by integrating features of adjacent video frames to simply encourage temporal constancy for image-level lane detectors. However, most of them ignore lane shape constraint of adjacent frames and geometry consistency of individual lanes, thereby harming the performance of video instance lane detection. In this paper, we propose TGC-Net via temporal and geometry consistency constraints for reliable video instance lane detection. Specifically, we devise a temporal recurrent feature-shift aggregation module (T-RESA) to learn spatio-temporal lane features along horizontal, vertical, and temporal directions of the feature tensor. We further impose temporal consistency constraint by encouraging spatial distribution consistency among the lane features of adjacent frames. Besides, we devise two effective geometry constraints to ensure the integrity and continuity of lane predictions by leveraging pairwise point affinity loss and vanishing point guided geometric context, respectively. Extensive experiments on public benchmark dataset show that our TGC-Net quantitatively and qualitatively outperforms state-of-the-art video instance lane detectors and video object segmentation competitors. Our code and our results have been released at https://github.com/wmq12345/TGC-Net.},
	language = {en},
	urldate = {2024-05-22},
	booktitle = {Proceedings of the 30th {ACM} {International} {Conference} on {Multimedia}},
	publisher = {ACM},
	author = {Wang, Mingqian and Zhang, Yujun and Feng, Wei and Zhu, Lei and Wang, Song},
	month = oct,
	year = {2022},
	keywords = {/unread},
	pages = {2324--2332},
	file = {Wang ç­‰ - 2022 - Video Instance Lane Detection via Deep Temporal an.pdf:D\:\\bw_ch\\Zotero\\storage\\B4LAPIIZ\\Wang ç­‰ - 2022 - Video Instance Lane Detection via Deep Temporal an.pdf:application/pdf},
}

@article{wang2018lanenet,
	title = {{LaneNet}: {Real}-{Time} {Lane} {Detection} {Networks} for {Autonomous} {Driving}},
	volume = {abs/1807.01726},
	language = {en},
	journal = {ArXiv},
	author = {Wang, Ze and Ren, Weiqiang and Qiu, Qiang},
	year = {2018},
	keywords = {/unread},
}

@inproceedings{xiao2023adnet,
	title = {{ADNet}: {Lane} {Shape} {Prediction} via {Anchor} {Decomposition}.},
	language = {en},
	booktitle = {{IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Xiao, Lingyu and Li, Xiang and Yang, Sen and Yang, Wankou},
	year = {2023},
	keywords = {/unread},
	pages = {6381--6390},
}

@inproceedings{xu2020curvelanenas,
	title = {{CurveLane}-{NAS}: {Unifying} {Lane}-{Sensitive} {Architecture} {Search} and {Adaptive} {Point} {Blending}.},
	language = {en},
	booktitle = {European {Conference} on {Computer} {Vision} ({ECCV})},
	author = {Xu, Hang and Wang, Shaoju and Cai, Xinyue and Zhang, Wei and Liang, Xiaodan and Li, Zhenguo},
	year = {2020},
	keywords = {/unread},
	pages = {689--704},
}

@inproceedings{zheng2021resa,
	title = {{RESA}: {Recurrent} {Feature}-{Shift} {Aggregator} for {Lane} {Detection}.},
	language = {en},
	booktitle = {{AAAI} {Conference} on {Artificial} {Intelligence} ({AAAI})},
	author = {Zheng, Tu and Fang, Hao and Zhang, Yi and Tang, Wenjian and Yang, Zheng and Liu, Haifeng and Cai, Deng},
	year = {2021},
	keywords = {/unread},
	pages = {3547--3554},
}

@article{DuoBuWeiLianHeYinFaZhiNengQiCheChuangXinFaZhanZhanLue2020,
	title = {å¤šéƒ¨å§”è”åˆå°å‘ã€Šæ™ºèƒ½æ±½è½¦åˆ›æ–°å‘å±•æˆ˜ç•¥ã€‹},
	issn = {1004-1230},
	url = {https://kns.cnki.net/kcms2/article/abstract?v=axnrJTP8flxDSIHEp2Bjz0n0yD1AZ4lFNzEgxxHz6KGMGe9V8uKz3_lZsE-wgTm1ls-6-4BtK9B9hKWgDiEQVva8K_-zYoBW7qlR_ywg0zMEXctjDpkNuHX0dBdlNj9vzOrvUYkSztSeSNzdr25taw==&uniplatform=NZKPT&language=CHS},
	abstract = {{\textless}æ­£{\textgreater}ä¸ºæ·±å…¥è´¯å½»è½å®å…šä¸­å¤®ã€å›½åŠ¡é™¢é‡è¦éƒ¨ç½²,é¡ºåº”æ–°ä¸€è½®ç§‘æŠ€é©å‘½å’Œäº§ä¸šå˜é©è¶‹åŠ¿,æŠ“ä½äº§ä¸šæ™ºèƒ½åŒ–å‘å±•æˆ˜ç•¥æœºé‡,åŠ å¿«æ¨è¿›æ™ºèƒ½æ±½è½¦åˆ›æ–°å‘å±•,å›½å®¶å‘å±•æ”¹é©å§”ã€ä¸­å¤®ç½‘ä¿¡åŠã€ç§‘æŠ€éƒ¨ã€å·¥ä¸šå’Œä¿¡æ¯åŒ–éƒ¨ã€å…¬å®‰éƒ¨ã€è´¢æ”¿éƒ¨ã€è‡ªç„¶èµ„æºéƒ¨ã€ä½æˆ¿åŸä¹¡å»ºè®¾éƒ¨ã€äº¤é€šè¿è¾“éƒ¨ã€å•†åŠ¡éƒ¨ã€å¸‚åœºç›‘ç®¡æ€»å±€è”åˆåˆ¶å®šäº†ã€Šæ™ºèƒ½æ±½è½¦åˆ›æ–°å‘å±•æˆ˜ç•¥ã€‹ã€‚ç°å°å‘ä½ ä»¬,è¯·ç»“åˆå®é™…åˆ¶å®šä¿ƒè¿›æ™ºèƒ½æ±½è½¦åˆ›æ–°å‘å±•çš„æ”¿ç­–æªæ–½,ç€åŠ›æ¨åŠ¨å„é¡¹æˆ˜ç•¥ä»»åŠ¡æœ‰æ•ˆè½å®ã€‚å…¨æ–‡å¦‚ä¸‹:æ™ºèƒ½æ±½è½¦åˆ›æ–°å‘å±•æˆ˜ç•¥å½“ä»Šä¸–ç•Œæ­£ç»å†ç™¾å¹´æœªæœ‰ä¹‹å¤§},
	language = {zh},
	number = {1},
	urldate = {2024-05-22},
	journal = {å¹¿è¥¿èŠ‚èƒ½},
	year = {2020},
	keywords = {ã€Šæ™ºèƒ½æ±½è½¦åˆ›æ–°å‘å±•æˆ˜ç•¥ã€‹, åˆ›æ–°å‘å±•æˆ˜ç•¥, æ±½è½¦å¼ºå›½, æ™ºèƒ½æ±½è½¦},
	pages = {8--10},
	file = {2020_å¤šéƒ¨å§”è”åˆå°å‘ã€Šæ™ºèƒ½æ±½è½¦åˆ›æ–°å‘å±•æˆ˜ç•¥ã€‹.pdf:D\:\\bw_ch\\Zotero\\storage\\SWY9U7MH\\2020_å¤šéƒ¨å§”è”åˆå°å‘ã€Šæ™ºèƒ½æ±½è½¦åˆ›æ–°å‘å±•æˆ˜ç•¥ã€‹.pdf:application/pdf},
}

@article{QiCheJiaShiZiDongHuaFenJiBiaoZhunFaBu2020,
	title = {ã€Šæ±½è½¦é©¾é©¶è‡ªåŠ¨åŒ–åˆ†çº§ã€‹æ ‡å‡†å‘å¸ƒ},
	issn = {1004-6437},
	url = {https://kns.cnki.net/kcms2/article/abstract?v=axnrJTP8flwZhRCxXqkOB5dEvwlMPGSBvq_5EllQswtjz7I2zMBWu9SAcXCFZ967dPasB8WKA43jVFakUvcnaa3LIueV4Ap0hnaDgpGLkBwU-rZJ3_eooGQjTqxpzSejbuxj4Pj5QtonWM5YSx8FtQ==&uniplatform=NZKPT&language=CHS},
	abstract = {{\textless}æ­£{\textgreater}æ ¹æ®å›½å®¶æ ‡å‡†åŒ–ç®¡ç†å§”å‘˜ä¼šä¸‹è¾¾çš„å›½å®¶æ ‡å‡†åˆ¶ä¿®è®¢è®¡åˆ’,å·¥ä¿¡éƒ¨äº3æœˆ9æ—¥å‘å¸ƒã€Šæ±½è½¦é©¾é©¶è‡ªåŠ¨åŒ–åˆ†çº§ã€‹(æŠ¥æ‰¹ç¨¿)(ä»¥ä¸‹ç®€ç§°ã€Šåˆ†çº§ã€‹)æ¨èæ€§å›½å®¶æ ‡å‡†,å°†äº2021å¹´1æœˆ1æ—¥æ­£å¼å®æ–½ã€‚è¿™æ„å‘³ç€æˆ‘å›½å³å°†æ­£å¼æ‹¥æœ‰è‡ªå·±çš„è‡ªåŠ¨é©¾é©¶æ±½è½¦åˆ†çº§æ ‡å‡†,ä¹Ÿå°†ä¸ºæˆ‘å›½åç»­è‡ªåŠ¨é©¾é©¶ç›¸å…³æ³•å¾‹ã€æ³•è§„ã€å¼ºåˆ¶æ€§æ ‡å‡†çš„å‡ºå°æä¾›æ”¯æ’‘ã€‚ã€Šåˆ†çº§ã€‹å¯¹é©¾é©¶è‡ªåŠ¨åŒ–ã€é©¾é©¶è‡ªåŠ¨åŒ–ç³»ç»Ÿã€é©¾é©¶è‡ªåŠ¨åŒ–åŠŸèƒ½ã€è½¦è¾†æ¨ªå‘è¿åŠ¨æ§åˆ¶ã€ç›®æ ‡å’Œäº‹ä»¶æ¢æµ‹ä¸å“åº”ã€åŠ¨æ€é©¾é©¶ä»»åŠ¡æ¥ç®¡ã€è®¾è®¡è¿è¡ŒèŒƒå›´ã€æ¥ç®¡è¯·æ±‚ç­‰æœ¯è¯­å®šä¹‰è¿›},
	language = {zh},
	number = {2},
	urldate = {2024-05-22},
	journal = {æœºå™¨äººæŠ€æœ¯ä¸åº”ç”¨},
	year = {2020},
	keywords = {/unread, ã€Šæ±½è½¦é©¾é©¶è‡ªåŠ¨åŒ–åˆ†çº§ã€‹, é©¾é©¶è¾…åŠ©, æ±½è½¦é©¾é©¶},
	pages = {3},
	file = {2020_ã€Šæ±½è½¦é©¾é©¶è‡ªåŠ¨åŒ–åˆ†çº§ã€‹æ ‡å‡†å‘å¸ƒ.pdf:D\:\\bw_ch\\Zotero\\storage\\45A9SJ3R\\2020_ã€Šæ±½è½¦é©¾é©¶è‡ªåŠ¨åŒ–åˆ†çº§ã€‹æ ‡å‡†å‘å¸ƒ.pdf:application/pdf},
}

@article{chen2023bsnet,
	title = {{BSNet}: {Lane} {Detection} via {Draw} {B}-spline {Curves} {Nearby}},
	volume = {abs/2301.06910},
	language = {en},
	journal = {ArXiv},
	author = {Chen, Haoxin and Wang, Mengmeng and Liu, Yong},
	year = {2023},
	keywords = {/unread},
}

@misc{alabaComprehensiveSurveyDeep2022,
	title = {A {Comprehensive} {Survey} of {Deep} {Learning} {Multisensor} {Fusion}-based {3D} {Object} {Detection} for {Autonomous} {Driving}: {Methods}, {Challenges}, {Open} {Issues}, and {Future} {Directions}},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	shorttitle = {A {Comprehensive} {Survey} of {Deep} {Learning} {Multisensor} {Fusion}-based {3D} {Object} {Detection} for {Autonomous} {Driving}},
	url = {https://www.techrxiv.org/doi/full/10.36227/techrxiv.20443107.v1},
	doi = {10.36227/techrxiv.20443107.v1},
	abstract = {Autonomous driving requires accurate, robust, and fast decision-making perception systems to understand the driving environment. Object detection is critical in allowing the perception system to understand the environment. The perception systems, especially 2D object detection and classiï¬cation, have succeeded because of the emergence of deep learning (DL) in computer vision (CV) applications. However, 2D object detection lacks depth information, which is crucial to understanding the driving environment. Therefore, 3D object detection is fundamental for the perception system of autonomous driving and robotics applications to estimate the objectsâ€™ location and understand the driving environment. The CV community has been giving much attention recently to 3D object detection because of the growth of DL models and the need to know accurate locations of objects.},
	language = {en},
	urldate = {2024-05-22},
	author = {Alaba, Simegnew},
	month = aug,
	year = {2022},
	file = {Alaba - 2022 - A Comprehensive Survey of Deep Learning Multisenso.pdf:D\:\\bw_ch\\Zotero\\storage\\BZS8GQDS\\Alaba - 2022 - A Comprehensive Survey of Deep Learning Multisenso.pdf:application/pdf},
}

@article{abualsaud2021laneaf,
	title = {{LaneAF}: {Robust} {Multi}-{Lane} {Detection} {With} {Affinity} {Fields}.},
	volume = {6},
	language = {en},
	number = {4},
	journal = {IEEE Robotics and Automation Letters},
	author = {Abualsaud, Hala and Liu, Sean and Lu, David and Situ, Kenny and Rangesh, Akshay and Trivedi, Mohan M.},
	year = {2021},
	keywords = {/unread},
	pages = {7477--7484},
}

@article{chenMilestonesAutonomousDriving2023,
	title = {Milestones in {Autonomous} {Driving} and {Intelligent} {Vehicles}â€”{Part} {II}: {Perception} and {Planning}},
	volume = {53},
	issn = {2168-2232},
	shorttitle = {Milestones in {Autonomous} {Driving} and {Intelligent} {Vehicles}â€”{Part} {II}},
	url = {https://ieeexplore.ieee.org/abstract/document/10156892},
	doi = {10.1109/TSMC.2023.3283021},
	abstract = {A growing interest in autonomous driving (AD) and intelligent vehicles (IVs) is fueled by their promise for enhanced safety, efficiency, and economic benefits. While previous surveys have captured progress in this field, a comprehensive and forward-looking summary is needed. Our work fills this gap through three distinct articles. The first part, a â€œsurvey of surveysâ€ (SoS), outlines the history, surveys, ethics, and future directions of AD and IV technologies. The second part, â€œMilestones in AD and IVs Part I: Control, Computing System Design, Communication, high-definition map (HD map), Testing, and Human Behaviorsâ€ delves into the development of control, computing system, communication, HD map, testing, and human behaviors in IVs. This part, the third part, reviews perception and planning in the context of IVs. Aiming to provide a comprehensive overview of the latest advancements in AD and IVs, this work caters to both newcomers and seasoned researchers. By integrating the SoS and Part I, we offer unique insights and strive to serve as a bridge between past achievements and future possibilities in this dynamic field.},
	language = {en},
	number = {10},
	urldate = {2024-05-22},
	journal = {IEEE Transactions on Systems, Man, and Cybernetics: Systems},
	author = {Chen, Long and Teng, Siyu and Li, Bai and Na, Xiaoxiang and Li, Yuchen and Li, Zixuan and Wang, Jinjun and Cao, Dongpu and Zheng, Nanning and Wang, Fei-Yue},
	month = oct,
	year = {2023},
	note = {Conference Name: IEEE Transactions on Systems, Man, and Cybernetics: Systems},
	keywords = {/unread, Location awareness, Visualization, Autonomous driving (AD), communication, control, high-definition map (HD map), human behaviors, intelligent vehicles (IVs), Laser radar, perception, planning, Planning, Simultaneous localization and mapping, survey of surveys (SoS), Surveys, system design, testing, Testing},
	pages = {6401--6415},
	file = {Chen et al_2023_Milestones in Autonomous Driving and Intelligent Vehiclesâ€”Part II.pdf:D\:\\bw_ch\\Zotero\\storage\\DTZCG9CY\\Chen et al_2023_Milestones in Autonomous Driving and Intelligent Vehiclesâ€”Part II.pdf:application/pdf;IEEE Xplore Abstract Record:D\:\\bw_ch\\Zotero\\storage\\P72HDBF8\\10156892.html:text/html},
}
