@book{蒋有绪1998,
  title={中国森林群落分类及其群落学特征},
  author={蒋有绪 and 郭泉水 and 马娟 and others},
  year={1998},
  publisher={科学出版社},
  address={北京},
  pages={11-12},
}

@inproceedings{中国力学学会1990,
  title={第3届全国实验流体力学学术会议论文集},
  author={中国力学学会},
  year={1990},
  address={天津},
  publisher={**出版社},
  pages={20-24}
}

@techreport{WHO1970,
  title={World Health Organization. Factors Regulating the Immune Response: Report of WHO Scientific Group},
  year={1970},
  institution={WHO},
  address={Geneva}
}

@phdthesis{张志祥1998,
  author = {张志祥},
  title = {间断动力系统的随机扰动及其在守恒律方程中的应用},
  school = {北京大学数学学院},
  address = {北京},
  year = {1998},
  pages = {50-55}
}

@patent{河北绿洲2001,
  title={一种荒漠化地区生态植被综合培育种植方法:中国，01129210.5},
  author={河北绿洲生态环境科技有限公司},
  year={2001},
  type={patent},
  number={01129210.5},
  note={2001-10-24[2002-05-28]},
  url={http://211.152.9.47/sipoasp/zlijs/hyjs-yxnew. asp?recid=01129210.5&leixin}
}

@standard{GB/T2659-1986,
  title = {世界各国和地区名称代码},
  author = {国家标准局信息分类编码研究所},
  year = {1986},
  howpublished = {全国文献工作标准化技术委员会. 文献工作国家标准汇编:3.北京:中国标准出版社，1988:59-92.},
  note = {GB/T 2659-1986}
}


@article{李炳穆2000,
  title={理想的图书馆员和信息专家的素质与形象},
  author={李炳穆},
  journal={图书情报工作},
  year={2000},
  volume={2000},
  number={2},
  pages={5-8}
}

@article{丁文祥2000,
  title={数字革命与竞争国际化},
  author={丁文祥},
  journal={中国青年报},
  year={2000},
  month={11-20},
  number={15}
}

@article{江向东1999,
  title={互联网环境下的信息处理与图书管理系统解决方案},
  author={江向东},
  journal={情报学报},
  year={1999},
  volume={18},
  number={2},
  pages={4},
  note={2000-01-18},
  url={http://www.chinainfo.gov.cn/periodical/gbxb/gbxb99/gbxb990203}
}

@article{CHRISTINE1998,
  title={Plant physiology:plant biology in the Genome Era},
  author={M CHRISTINE},
  journal={Science},
  year={1998},
  volume={281},
  pages={331-332},
  note={1998-09-23},
  url={http://www.sciencemag.org/cgi/collection/anatmorp}
}


@inproceedings{zheng2022clrnet,
	address = {New Orleans, LA, USA},
	title = {{CLRNet}: {Cross} {Layer} {Refinement} {Network} for {Lane} {Detection}},
	isbn = {978-1-66546-946-3},
	shorttitle = {{CLRNet}},
	url = {https://ieeexplore.ieee.org/document/9879280/},
	doi = {10.1109/CVPR52688.2022.00097},
	abstract = {Lane is critical in the vision navigation system of the intelligent vehicle. Naturally, lane is a traffic sign with high-level semantics, whereas it owns the specific local pattern which needs detailed low-level features to localize accurately. Using different feature levels is of great importance for accurate lane detection, but it is still underexplored. In this work, we present Cross Layer Refinement Network (CLRNet) aiming at fully utilizing both highlevel and low-level features in lane detection. In particular, it first detects lanes with high-level semantic features then performs refinement based on low-level features. In this way, we can exploit more contextual information to detect lanes while leveraging local detailed lane features to improve localization accuracy. We present ROIGather to gather global context, which further enhances the feature representation of lanes. In addition to our novel network design, we introduce Line IoU loss which regresses the lane line as a whole unit to improve the localization accuracy. Experiments demonstrate that the proposed method greatly outperforms the state-of-the-art lane detection approaches.},
	language = {en},
	urldate = {2022-11-14},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Zheng, Tu and Huang, Yifei and Liu, Yang and Tang, Wenjian and Yang, Zheng and Cai, Deng and He, Xiaofei},
	month = jun,
	year = {2022},
	note = {TLDR: This work presents Cross Layer Refinement Network (CLRNet), a novel network design which first detects lanes with high-level semantic features then performs refinement based on low-level features and introduces Line IoU loss which regresses the lane line as a whole unit to improve the localization accuracy.},
	keywords = {/unread, 📒, ⛔ No INSPIRE recid found},
	pages = {888--897},
	file = {Zheng 等 - 2022 - CLRNet Cross Layer Refinement Network for Lane De.pdf:D\:\\bw_ch\\Zotero\\storage\\PMLECK6C\\Zheng 等 - 2022 - CLRNet Cross Layer Refinement Network for Lane De.pdf:application/pdf},
}

@article{liPETDetProposalEnhancement2023,
	title = {{PETDet}: {Proposal} {Enhancement} for {Two}-{Stage} {Fine}-{Grained} {Object} {Detection}},
	volume = {62},
	issn = {0196-2892, 1558-0644},
	shorttitle = {{PETDet}},
	url = {http://arxiv.org/abs/2312.10515},
	doi = {10/gtd5pk},
	abstract = {Fine-grained object detection (FGOD) extends object detection with the capability of fine-grained recognition. In recent two-stage FGOD methods, the region proposal serves as a crucial link between detection and fine-grained recognition. However, current methods overlook that some proposal-related procedures inherited from general detection are not equally suitable for FGOD, limiting the multi-task learning from generation, representation, to utilization. In this paper, we present PETDet (Proposal Enhancement for Two-stage fine-grained object detection) to better handle the sub-tasks in two-stage FGOD methods. Firstly, an anchor-free Quality Oriented Proposal Network (QOPN) is proposed with dynamic label assignment and attention-based decomposition to generate high-quality oriented proposals. Additionally, we present a Bilinear Channel Fusion Network (BCFN) to extract independent and discriminative features of the proposals. Furthermore, we design a novel Adaptive Recognition Loss (ARL) which offers guidance for the R-CNN head to focus on high-quality proposals. Extensive experiments validate the effectiveness of PETDet. Quantitative analysis reveals that PETDet with ResNet50 reaches state-of-the-art performance on various FGOD datasets, including FAIR1M-v1.0 (42.96 AP), FAIR1M-v2.0 (48.81 AP), MAR20 (85.91 AP) and ShipRSImageNet (74.90 AP). The proposed method also achieves superior compatibility between accuracy and inference speed. Our code and models will be released at https://github.com/canoe-Z/PETDet.},
	language = {en},
	urldate = {2023-12-27},
	journal = {IEEE Transactions on Geoscience and Remote Sensing},
	author = {Li, Wentao and Zhao, Danpei and Yuan, Bo and Gao, Yue and Shi, Zhenwei},
	year = {2023},
	note = {arXiv:2312.10515 [cs]
JCR分区: Q1
中科院分区升级版: 地球科学1区
影响因子: 7.5
5年影响因子: 7.6
EI: 是
CCF: B
中国地质大学: 工程技术T2},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, /unread},
	pages = {1--1},
	file = {arXiv.org Snapshot:D\:\\bw_ch\\Zotero\\storage\\TVWQD4BH\\2312.html:text/html;arXiv.org Snapshot:D\:\\bw_ch\\Zotero\\storage\\G7DUHD6V\\2312.html:text/html;Li et al_2023_PETDet.pdf:D\:\\bw_ch\\Zotero\\storage\\V59WQC5V\\Li et al_2023_PETDet.pdf:application/pdf},
}

@inproceedings{wang2022keypointbased,
	address = {New Orleans, LA, USA},
	title = {A {Keypoint}-based {Global} {Association} {Network} for {Lane} {Detection}},
	isbn = {978-1-66546-946-3},
	url = {https://ieeexplore.ieee.org/document/9879757/},
	doi = {10.1109/CVPR52688.2022.00145},
	abstract = {Lane detection is a challenging task that requires predicting complex topology shapes of lane lines and distinguishing different types of lanes simultaneously. Earlier works follow a top-down roadmap to regress predefined anchors into various shapes of lane lines, which lacks enough flexibility to fit complex shapes of lanes due to the fixed anchor shapes. Lately, some works propose to formulate lane detection as a keypoint estimation problem to describe the shapes of lane lines more flexibly and gradually group adjacent keypoints belonging to the same lane line in a pointby-point manner, which is inefficient and time-consuming during postprocessing. In this paper, we propose a Global Association Network (GANet) to formulate the lane detection problem from a new perspective, where each keypoint is directly regressed to the starting point of the lane line instead of point-by-point extension. Concretely, the association of keypoints to their belonged lane line is conducted by predicting their offsets to the corresponding starting points of lanes globally without dependence on each other, which could be done in parallel to greatly improve efficiency. In addition, we further propose a Lane-aware Feature Aggregator (LFA), which adaptively captures the local correlations between adjacent keypoints to supplement local information to the global association. Extensive experiments on two popular lane detection benchmarks show that our method outperforms previous methods with F1 score of 79.63\% on CULane and 97.71\% on Tusimple dataset with high FPS. The code will be released at https://github.com/Wolfwjs/GANet.},
	language = {en},
	urldate = {2022-11-14},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Wang, Jinsheng and Ma, Yinchao and Huang, Shaofei and Hui, Tianrui and Wang, Fei and Qian, Chen and Zhang, Tianzhu},
	month = jun,
	year = {2022},
	note = {TLDR: A Global Association Network (GANet) is proposed to formulate the lane detection problem from a new perspective, where each keypoint is directly regressed to the starting point of the lane line instead of point-by-point extension, which outperforms previous methods.},
	keywords = {/unread},
	pages = {1382--1391},
	file = {Wang 等 - 2022 - A Keypoint-based Global Association Network for La.pdf:D\:\\bw_ch\\Zotero\\storage\\8G7EDWDV\\Wang 等 - 2022 - A Keypoint-based Global Association Network for La.pdf:application/pdf},
}

@inproceedings{lin2017feature,
	address = {Honolulu, HI},
	title = {Feature {Pyramid} {Networks} for {Object} {Detection}},
	isbn = {978-1-5386-0457-1},
	url = {http://ieeexplore.ieee.org/document/8099589/},
	doi = {10.1109/CVPR.2017.106},
	abstract = {Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But recent deep learning object detectors have avoided pyramid representations, in part because they are compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A topdown architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network (FPN), shows signiﬁcant improvement as a generic feature extractor in several applications. Using FPN in a basic Faster R-CNN system, our method achieves state-of-the-art singlemodel results on the COCO detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the COCO 2016 challenge winners. In addition, our method can run at 5 FPS on a GPU and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available.},
	language = {en},
	urldate = {2024-03-07},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Lin, Tsung-Yi and Dollar, Piotr and Girshick, Ross and He, Kaiming and Hariharan, Bharath and Belongie, Serge},
	month = jul,
	year = {2017},
	keywords = {/unread},
	pages = {936--944},
	file = {Lin 等 - 2017 - Feature Pyramid Networks for Object Detection.pdf:D\:\\bw_ch\\Zotero\\storage\\KPWAITKE\\Lin 等 - 2017 - Feature Pyramid Networks for Object Detection.pdf:application/pdf},
}

@inproceedings{zhangVIL100NewDataset2021,
	title = {{VIL}-100: {A} {New} {Dataset} and {A} {Baseline} {Model} for {Video} {Instance} {Lane} {Detection}},
	shorttitle = {{VIL}-100},
	url = {https://ieeexplore.ieee.org/document/9710077},
	doi = {10/gp8m7x},
	abstract = {Lane detection plays a key role in autonomous driving. While car cameras always take streaming videos on the way, current lane detection works mainly focus on individual images (frames) by ignoring dynamics along the video. In this work, we collect a new video instance lane detection (VIL-100) dataset, which contains 100 videos with in total 10,000 frames, acquired from different real traffic scenarios. All the frames in each video are manually annotated to a high-quality instance-level lane annotation, and a set of frame-level and video-level metrics are included for quantitative performance evaluation. Moreover, we propose a new baseline model, named multi-level memory aggregation network (MMA-Net), for video instance lane detection. In our approach, the representation of current frame is enhanced by attentively aggregating both local and global memory features from other frames. Experiments on the new collected dataset show that the proposed MMA-Net outperforms state-of-the-art lane detection methods and video object segmentation methods. We release our dataset and code at https://github.com/yujun0-0/MMA-Net.},
	language = {en},
	urldate = {2024-01-17},
	booktitle = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Zhang, Yujun and Zhu, Lei and Feng, Wei and Fu, Huazhu and Wang, Mingqian and Li, Qingxia and Li, Cheng and Wang, Song},
	month = oct,
	year = {2021},
	note = {ISSN: 2380-7504},
	keywords = {/unread},
	pages = {15661--15670},
	file = {IEEE Xplore Abstract Record:D\:\\bw_ch\\Zotero\\storage\\35I4UD44\\9710077.html:text/html;Zhang et al_2021_VIL-100.pdf:D\:\\bw_ch\\Zotero\\storage\\DG25B5IW\\Zhang et al_2021_VIL-100.pdf:application/pdf},
}

@article{panSpatialDeepSpatial2018,
	title = {Spatial as {Deep}: {Spatial} {CNN} for {Traffic} {Scene} {Understanding}},
	volume = {32},
	copyright = {Copyright (c)},
	issn = {2374-3468},
	shorttitle = {Spatial as {Deep}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/12301},
	doi = {10/gqzs6n},
	abstract = {Convolutional neural networks (CNNs) are usually built by stacking convolutional operations layer-by-layer. Although CNN has shown strong capability to extract semantics from raw pixels, its capacity to capture spatial relationships of pixels across rows and columns of an image is not fully explored. These relationships are important to learn semantic objects with strong shape priors but weak appearance coherences, such as traffic lanes, which are often occluded or not even painted on the road surface as shown in Fig. 1 (a). In this paper, we propose Spatial CNN (SCNN), which generalizes traditional deep layer-by-layer convolutions to slice-by-slice convolutions within feature maps, thus enabling message passings between pixels across rows and columns in a layer. Such SCNN is particular suitable for long continuous shape structure or large objects, with strong spatial relationship but less appearance clues, such as traffic lanes, poles, and wall. We apply SCNN on a newly released very challenging traffic lane detection dataset and Cityscapse dataset. The results show that SCNN could learn the spatial relationship for structure output and significantly improves the performance. We show that SCNN outperforms the recurrent neural network (RNN) based ReNet and MRF+CNN (MRFNet) in the lane detection dataset by 8.7\% and 4.6\% respectively. Moreover, our SCNN won the 1st place on the TuSimple Benchmark Lane Detection Challenge, with an accuracy of 96.53\%.},
	language = {en},
	number = {1},
	urldate = {2024-01-17},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Pan, Xingang and Shi, Jianping and Luo, Ping and Wang, Xiaogang and Tang, Xiaoou},
	month = apr,
	year = {2018},
	note = {Number: 1},
	keywords = {/unread, Spatial CNN},
	file = {Pan et al_2018_Spatial as Deep.pdf:D\:\\bw_ch\\Zotero\\storage\\66N5BXUL\\Pan et al_2018_Spatial as Deep.pdf:application/pdf},
}

@article{liLineCNNEndEndTraffic2020,
	title = {Line-{CNN}: {End}-to-{End} {Traffic} {Line} {Detection} {With} {Line} {Proposal} {Unit}},
	volume = {21},
	issn = {1558-0016},
	shorttitle = {Line-{CNN}},
	url = {https://ieeexplore.ieee.org/document/8624563},
	doi = {10/ghfvqj},
	abstract = {The task of traffic line detection is a fundamental yet challenging problem. Previous approaches usually conduct traffic line detection via a two-stage way, namely the line segment detection followed by a segment clustering, which is very likely to ignore the global semantic information of an entire line. To address the problem, we propose an end-to-end system called Line-CNN (L-CNN), in which the key component is a novel line proposal unit (LPU). The LPU utilizes line proposals as references to locate accurate traffic curves, which forces the system to learn the global feature representation of the entire traffic lines. We benchmark the proposed L-CNN on two public datasets including MIKKI and TuSimple, and the results suggest that L-CNN outperforms the state-of-the-art methods. In addition, L-CNN can run at approximately 30 f/s on a Titan X GPU, which indicates the practicability and effectiveness of L-CNN for real-time intelligent self-driving systems.},
	language = {en},
	number = {1},
	urldate = {2024-01-21},
	journal = {IEEE Transactions on Intelligent Transportation Systems},
	author = {Li, Xiang and Li, Jun and Hu, Xiaolin and Yang, Jian},
	month = jan,
	year = {2020},
	note = {Conference Name: IEEE Transactions on Intelligent Transportation Systems},
	keywords = {/unread},
	pages = {248--258},
	file = {IEEE Xplore Abstract Record:D\:\\bw_ch\\Zotero\\storage\\V48TJ2NM\\8624563.html:text/html;Li et al_2020_Line-CNN.pdf:D\:\\bw_ch\\Zotero\\storage\\YHP3KWQZ\\Li et al_2020_Line-CNN.pdf:application/pdf;Li et al_2020_Line-CNN.pdf:D\:\\bw_ch\\Zotero\\storage\\KEMFMEQD\\Li et al_2020_Line-CNN.pdf:application/pdf},
}

@article{tabeliniLaneMarkingDetection2022,
	title = {Lane {Marking} {Detection} and {Classification} using {Spatial}-{Temporal} {Feature} {Pooling}},
	url = {https://ieeexplore.ieee.org/document/9892478},
	doi = {10.1109/IJCNN55064.2022.9892478},
	abstract = {The lane detection problem has been extensively researched in the past decades, especially since the advent of deep learning. Despite the numerous works proposing solutions to the localization task (i.e., localizing the lane boundaries in an input image), the classification task has not seen the same focus. Nonetheless, knowing the type of lane boundary, particularly that of the ego lane, can be very useful for many applications. For instance, a vehicle might not be allowed by law to overtake depending on the type of the ego lane. Beyond that, very few works take advantage of the temporal information available in the videos captured by the vehicles: most methods employ a single-frame approach. In this work, building upon the recent LaneATT model, we propose an approach to exploit the temporal information and integrate the classification task into the model. Our results show that the proposed modifications can improve the detection performance on the most recent benchmark by 2.34 \%, establishing a new state-of-the-art. Finally, an extensive evaluation shows that it enables a high classification performance (89.37 \%) that serves as a future benchmark for the field.},
	language = {en},
	urldate = {2024-05-22},
	journal = {2022 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {Tabelini, Lucas and Berriel, Rodrigo and De Souza, Alberto F. and Badue, Claudine and Oliveira-Santos, Thiago},
	month = jul,
	year = {2022},
	note = {ISSN: 2161-4407},
	keywords = {/unread, Deep learning, deep learning, Benchmark testing, lane classification, lane detection, Lane detection, Location awareness, Neural networks, Predictive models, temporal information, Transforms},
	pages = {1--7},
	file = {IEEE Xplore Abstract Record:D\:\\bw_ch\\Zotero\\storage\\FMVBQI5W\\9892478.html:text/html;Tabelini et al_2022_Lane Marking Detection and Classification using Spatial-Temporal Feature Pooling.pdf:D\:\\bw_ch\\Zotero\\storage\\TVK3D2JU\\Tabelini et al_2022_Lane Marking Detection and Classification using Spatial-Temporal Feature Pooling.pdf:application/pdf},
}

@article{zhangLaneDetectionModel2022,
	title = {Lane {Detection} {Model} {Based} on {Spatio}-{Temporal} {Network} {With} {Double} {Convolutional} {Gated} {Recurrent} {Units}},
	volume = {23},
	issn = {1558-0016},
	url = {https://ieeexplore.ieee.org/document/9364924},
	doi = {10.1109/TITS.2021.3060258},
	abstract = {Lane detection is one of the indispensable and key elements of self-driving environmental perception. Many lane detection models have been proposed, solving lane detection under challenging conditions, including intersection merging and splitting, curves, boundaries, occlusions and combinations of scene types. Nevertheless, lane detection will remain an open problem for some time to come. The ability to cope well with those challenging scenes impacts greatly the applications of lane detection on advanced driver assistance systems (ADASs). In this paper, a spatio-temporal network with double Convolutional Gated Recurrent Units (ConvGRUs) is proposed to address lane detection in challenging scenes. Both of ConvGRUs have the same structures, but different locations and functions in our network. One is used to extract the information of the most likely low-level features of lane markings. The extracted features are input into the next layer of the end-to-end network after concatenating them with the outputs of some blocks. The other one takes some continuous frames as its input to process the spatio-temporal driving information. Extensive experiments on the large-scale TuSimple lane marking challenge dataset and Unsupervised LLAMAS dataset demonstrate that the proposed model can effectively detect lanes in the challenging driving scenes. Our model can outperform the state-of-the-art lane detection models.},
	language = {en},
	number = {7},
	urldate = {2024-05-18},
	journal = {IEEE Transactions on Intelligent Transportation Systems},
	author = {Zhang, Jiyong and Deng, Tao and Yan, Fei and Liu, Wenbo},
	month = jul,
	year = {2022},
	note = {Conference Name: IEEE Transactions on Intelligent Transportation Systems
TLDR: A spatio-temporal network with double Convolutional Gated Recurrent Units (ConvGRUs) is proposed to address lane detection in challenging scenes and can outperform the state-of-the-art lane detection models.},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, /unread, Deep learning, Feature extraction, Lane detection, ConvGRUs, convolutional neural network, end-to-end, Image segmentation, Logic gates, Roads, Semantics, spatio-temporal},
	pages = {6666--6678},
	file = {arXiv.org Snapshot:D\:\\bw_ch\\Zotero\\storage\\3ENFZAMD\\2008.html:text/html;Zhang et al_2022_Lane Detection Model Based on Spatio-Temporal Network With Double Convolutional.pdf:D\:\\bw_ch\\Zotero\\storage\\ICIYJGFV\\Zhang et al_2022_Lane Detection Model Based on Spatio-Temporal Network With Double Convolutional.pdf:application/pdf},
}

@article{zouRobustLaneDetection2020,
	title = {Robust {Lane} {Detection} from {Continuous} {Driving} {Scenes} {Using} {Deep} {Neural} {Networks}},
	volume = {69},
	issn = {0018-9545, 1939-9359},
	url = {http://arxiv.org/abs/1903.02193},
	doi = {10.1109/TVT.2019.2949603},
	abstract = {Lane detection in driving scenes is an important module for autonomous vehicles and advanced driver assistance systems. In recent years, many sophisticated lane detection methods have been proposed. However, most methods focus on detecting the lane from one single image, and often lead to unsatisfactory performance in handling some extremely-bad situations such as heavy shadow, severe mark degradation, serious vehicle occlusion, and so on. In fact, lanes are continuous line structures on the road. Consequently, the lane that cannot be accurately detected in one current frame may potentially be inferred out by incorporating information of previous frames. To this end, we investigate lane detection by using multiple frames of a continuous driving scene, and propose a hybrid deep architecture by combining the convolutional neural network (CNN) and the recurrent neural network (RNN). Specifically, information of each frame is abstracted by a CNN block, and the CNN features of multiple continuous frames, holding the property of time-series, are then fed into the RNN block for feature learning and lane prediction. Extensive experiments on two large-scale datasets demonstrate that, the proposed method outperforms the competing methods in lane detection, especially in handling difficult situations.},
	language = {en},
	number = {1},
	urldate = {2024-05-18},
	journal = {IEEE Transactions on Vehicular Technology},
	author = {Zou, Qin and Jiang, Hanwen and Dai, Qiyu and Yue, Yuanhao and Chen, Long and Wang, Qian},
	month = jan,
	year = {2020},
	note = {arXiv:1903.02193 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, /unread},
	pages = {41--54},
	file = {arXiv.org Snapshot:D\:\\bw_ch\\Zotero\\storage\\WPNTN9MA\\1903.html:text/html;Zou et al_2020_Robust Lane Detection from Continuous Driving Scenes Using Deep Neural Networks.pdf:D\:\\bw_ch\\Zotero\\storage\\XCPGMC2Q\\Zou et al_2020_Robust Lane Detection from Continuous Driving Scenes Using Deep Neural Networks.pdf:application/pdf},
}

@inproceedings{dai2021attentional,
	title = {Attentional {Feature} {Fusion}},
	url = {https://openaccess.thecvf.com/content/WACV2021/html/Dai_Attentional_Feature_Fusion_WACV_2021_paper.html},
	language = {en},
	urldate = {2024-02-29},
	booktitle = {{IEEE}/{CVF} {Winter} {Conference} on {Applications} of {Computer} {Vision} ({WACV})},
	author = {Dai, Yimian and Gieseke, Fabian and Oehmcke, Stefan and Wu, Yiquan and Barnard, Kobus},
	year = {2021},
	keywords = {/unread},
	pages = {3560--3569},
	file = {Dai et al_2021_Attentional Feature Fusion.pdf:D\:\\bw_ch\\Zotero\\storage\\GGL4K4Q8\\Dai et al_2021_Attentional Feature Fusion.pdf:application/pdf},
}

@article{liang2020lane,
	title = {Lane {Detection}: a {Survey} with {New} {Results}},
	volume = {35},
	language = {en},
	number = {3},
	journal = {Journal of Computer Science and Technology},
	author = {Liang, Dun and Guo, Yuan-Chen and Zhang, Shao-Kui and Mu, Tai-Jiang and Huang, Xiaolei},
	year = {2020},
	note = {Publisher: Springer Science and Business Media LLC},
	keywords = {/unread},
	pages = {493--505},
}

@article{YinZhangCaiZiDongJiaShiGaoJingDiTuDeXinXiChuanShuMoXing2024,
	title = {自动驾驶高精地图的信息传输模型},
	volume = {49},
	issn = {1671-8860},
	url = {https://kns.cnki.net/kcms2/article/abstract?v=axnrJTP8flwIIog7bfPjWLX9aiyRtZbrG_-hCEGoEIqm_CFj25YOCJF2F_aE4lhbNF0LKF0pkdIZ99jB-BL-E5IRL-a3uee5JFMJ2CI_Mv5ikwYrmLv_G8YG2aIGMsqWkrVSOpzKO7lVTfvnyE_Gcw==&uniplatform=NZKPT&language=CHS},
	doi = {10.13203/j.whugis20230135},
	abstract = {高精地图的“非视觉”和面向机器特性，使其与传统面向人的时空产品有明显不同，相应的描述地图主、客体及其与产品之间关系的传输模型也面临巨大变革。已有的传输模型重构了上述关系，包括新增用户个性信息及其传输，但也存在信息传递工具仍采用面向人而非机器的地图语言等不足。为此，结合自动驾驶中地图信息的传输特点，构建面向机器认知的高精地图信息传输模型，通过对已有的传输模型进行扩展：GIS语言代替地图语言，将用户个性信息整合到高精地图的用户图层中，将行动指导扩展为行动实践。研究结果表明,构建的全机器认知的高精地图信息传输模型实现了地图信息认知的主体由人到机器的扩展，适应了高精地图在感知、定位、规划、控制等传输过程中全人工智能的特性。所提出的模型一方面有助于准确把握高精地图的本质及内容结构，提升认知效果；另一方面，对驾驶服务具有重要的指导作用，包括内容选取、表达方法、系统功能框架设计等，提高传输效率。},
	language = {zh},
	number = {4},
	urldate = {2024-05-22},
	journal = {武汉大学学报(信息科学版)},
	author = {{尹章才} and {齐如煜} and {应申}},
	year = {2024},
	keywords = {/unread, 地图空间认知, 地图信息传输模型, 高精地图, 自动驾驶},
	pages = {527--536},
}

@inproceedings{cai2018cascade,
	title = {Cascade {R}-{CNN}: {Delving} {Into} {High} {Quality} {Object} {Detection}.},
	language = {en},
	booktitle = {Computer {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Cai, Zhaowei and Vasconcelos, Nuno},
	year = {2018},
	keywords = {/unread},
	pages = {6154--6162},
}

@article{ZangJinHuanXinNengYuanQiCheChanYeFaZhanGuiHua20212035NianDiaoZhengJieDu2021,
	title = {《新能源汽车产业发展规划(2021—2035年)》调整解读},
	issn = {2095-9044},
	url = {https://kns.cnki.net/kcms2/article/abstract?v=axnrJTP8flzVDV_4zpnHJrtTvp0I62PSoywJYXyolsK6san85xDaTLNyGCiLlybAgiaur5yQcox4NCVX_O4Ed8KAl1ozIf5sRnoD4mMVsn4MId8S0roZczzm3JLTc_ZfYGRo9ZbsIW8mYya8c07b2A==&uniplatform=NZKPT&language=CHS},
	doi = {10.16173/j.cnki.ame.2021.z1.019},
	abstract = {{\textless}正{\textgreater}2020年11月2日,国务院办公厅发布《新能源汽车产业发展规划(2021—2035年)》(以下简称《规划》),这是继《节能与新能源汽车产业发展规划(2012—2020年)》后,我国关于新能源汽车产业的又一纲领性文件,明确了中国新能源汽车发展的愿景和相应部署。值得一提的是,由于油耗问题已经有相应的标准和要求,所以此次《规划》并未涉及传统燃油汽车的节能问题,而是把重点聚焦到新能源汽车产业的发展。},
	language = {zh},
	number = {Z1},
	urldate = {2024-05-22},
	journal = {汽车工艺师},
	author = {{臧金环} and {李春玲}},
	year = {2021},
	keywords = {/unread, 《新能源汽车产业发展规划(2021—2035年)》, 充电基础设施, 氢燃料汽车, 新能源汽车产业, 信息通信融合, 智能网联汽车},
	pages = {32--34},
	file = {臧金环_李春玲_2021_《新能源汽车产业发展规划(2021—2035年)》调整解读.pdf:D\:\\bw_ch\\Zotero\\storage\\KVMWBQDJ\\臧金环_李春玲_2021_《新能源汽车产业发展规划(2021—2035年)》调整解读.pdf:application/pdf},
}

@inproceedings{feng2022rethinking,
	title = {Rethinking {Efficient} {Lane} {Detection} via {Curve} {Modeling}.},
	language = {en},
	booktitle = {Computer {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Feng, Zhengyang and Guo, Shaohua and Tan, Xin and Xu, Ke and Wang, Min and Ma, Lizhuang},
	year = {2022},
	keywords = {/unread},
	pages = {17041--17049},
}

@article{jeongTutorialHighDefinitionMap2022,
	title = {Tutorial on {High}-{Definition} {Map} {Generation} for {Automated} {Driving} in {Urban} {Environments}},
	volume = {22},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/22/18/7056},
	doi = {10.3390/s22187056},
	abstract = {High-definition (HD) mapping is a promising approach to realize highly automated driving (AD). Although HD maps can be applied to all levels of autonomy, their use is particularly beneficial for autonomy levels 4 or higher. HD maps enable AD systems to see beyond the field of view of conventional sensors, thereby providing accurate and detailed information regarding a driving environment. An HD map is typically separated into a pointcloud map for localization and a vector map for path planning. In this paper, we introduce two separate but successive HD map generation workflows. Of the several stages involved, the registration and mapping processes are essential for creating the pointcloud and vector maps, respectively. To facilitate the readers’ understanding, the processes of these two stages have been recorded and uploaded online. HD maps are typically generated using open-source software (OSS) tools. CloudCompare and ASSURE, as representative tools, are used in this study. The generated HD maps are validated with localization and path-planning modules in Autoware, which is also an OSS stack for AD systems. The generated HD maps enable environmental-monitoring vehicles to successfully operate at level 4 autonomy.},
	language = {en},
	number = {18},
	urldate = {2024-05-22},
	journal = {Sensors},
	author = {Jeong, Jinseop and Yoon, Jun Yong and Lee, Hwanhong and Darweesh, Hatem and Sung, Woosuk},
	month = jan,
	year = {2022},
	note = {Number: 18
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {/unread, autonomous driving, high-definition map, localization, path planning},
	pages = {7056},
	file = {Jeong et al_2022_Tutorial on High-Definition Map Generation for Automated Driving in Urban.pdf:D\:\\bw_ch\\Zotero\\storage\\L3HWQHCK\\Jeong et al_2022_Tutorial on High-Definition Map Generation for Automated Driving in Urban.pdf:application/pdf},
}

@article{ebrahimisoorchaeiHighDefinitionMapRepresentation2022,
	title = {High-{Definition} {Map} {Representation} {Techniques} for {Automated} {Vehicles}},
	volume = {11},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2079-9292},
	url = {https://www.mdpi.com/2079-9292/11/20/3374},
	doi = {10.3390/electronics11203374},
	abstract = {Many studies in the field of robot navigation have focused on environment representation and localization. The goal of map representation is to summarize spatial information in topological and geometrical abstracts. By providing strong priors, maps improve the performance and reliability of automated robots. Due to the transition to fully automated driving in recent years, there has been a constant effort to design methods and technologies to improve the precision of road participants and the environment’s information. Among these efforts is the high-definition (HD) map concept. Making HD maps requires accuracy, completeness, verifiability, and extensibility. Because of the complexity of HD mapping, it is currently expensive and difficult to implement, particularly in an urban environment. In an urban traffic system, the road model is at least a map with sets of roads, lanes, and lane markers. While more research is being dedicated to mapping and localization, a comprehensive review of the various types of map representation is still required. This paper presents a brief overview of map representation, followed by a detailed literature review of HD maps for automated vehicles. The current state of autonomous vehicle (AV) mapping is encouraging, the field has matured to a point where detailed maps of complex environments are built in real time and have been proved useful. Many existing techniques are robust to noise and can cope with a large range of environments. Nevertheless, there are still open problems for future research. AV mapping will continue to be a highly active research area essential to the goal of achieving full autonomy.},
	language = {en},
	number = {20},
	urldate = {2024-05-22},
	journal = {Electronics},
	author = {Ebrahimi Soorchaei, Babak and Razzaghpour, Mahdi and Valiente, Rodolfo and Raftari, Arash and Fallah, Yaser Pourmohammadi},
	month = jan,
	year = {2022},
	note = {Number: 20
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {/unread, connected and automated vehicles, high-definition (HD) map, map representation, navigation},
	pages = {3374},
	file = {Ebrahimi Soorchaei et al_2022_High-Definition Map Representation Techniques for Automated Vehicles.pdf:D\:\\bw_ch\\Zotero\\storage\\75U7M9UE\\Ebrahimi Soorchaei et al_2022_High-Definition Map Representation Techniques for Automated Vehicles.pdf:application/pdf},
}

@inproceedings{jin2023recursive,
	title = {Recursive {Video} {Lane} {Detection}},
	language = {en},
	booktitle = {2023 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Jin, Dongkwon and Kim, Dahyun and Kim, Chang-Su},
	year = {2023},
	keywords = {/unread},
	pages = {8439--8448},
}

@article{ko2022points,
	title = {Key {Points} {Estimation} and {Point} {Instance} {Segmentation} {Approach} for {Lane} {Detection}.},
	volume = {23},
	language = {en},
	number = {7},
	journal = {IEEE Transactions on Intelligent Transportation Systems},
	author = {Ko, YeongMin and Lee, Younkwan and Azam, Shoaib and Munir, Farzeen and Jeon, Moongu and Pedrycz, Witold},
	year = {2022},
	keywords = {/unread},
	pages = {8949--8958},
}

@article{liuVisionbasedEnvironmentalPerception2023,
	title = {Vision-based environmental perception for autonomous driving},
	issn = {0954-4070},
	url = {https://doi.org/10.1177/09544070231203059},
	doi = {10.1177/09544070231203059},
	abstract = {Visual perception plays an important role in autonomous driving. One of the primary tasks is object detection and identification. Since the vision sensor is rich in color and texture information, it can quickly and accurately identify various road information. The commonly used technique is based on extracting and calculating various features of the image. The recent development of deep learning-based method has better reliability and processing speed and has a greater advantage in recognizing complex elements. For depth estimation, vision sensor is also used for ranging due to their small size and low cost. Monocular camera uses image data from a single viewpoint as input to estimate object depth. In contrast, stereo vision is based on parallax and matching feature points of different views, and the application of Deep learning also further improves the accuracy. In addition, Simultaneous Location and Mapping (SLAM) can establish a model of the road environment, thus helping the vehicle perceive the surrounding environment and complete the tasks. In this paper, we introduce and compare various methods of object detection and identification, then explain the development of depth estimation and compare various methods based on monocular, stereo, and RGB-D sensors, next review and compare various methods of SLAM, and finally summarize the current problems and present the future development trends of vision technologies.},
	language = {en},
	urldate = {2024-05-22},
	journal = {Proceedings of the Institution of Mechanical Engineers, Part D: Journal of Automobile Engineering},
	author = {Liu, Fei and Lu, Zihao and Lin, Xianke},
	month = nov,
	year = {2023},
	note = {Publisher: IMECHE},
	keywords = {/unread},
	pages = {09544070231203059},
	file = {Liu et al_2023_Vision-based environmental perception for autonomous driving.pdf:D\:\\bw_ch\\Zotero\\storage\\8YFKF7WW\\Liu et al_2023_Vision-based environmental perception for autonomous driving.pdf:application/pdf},
}

@inproceedings{liu2021endtoend,
	title = {End-to-end {Lane} {Shape} {Prediction} with {Transformers}.},
	language = {en},
	booktitle = {{IEEE}/{CVF} {Winter} {Conference} on {Applications} of {Computer} {Vision} ({WACV})},
	author = {Liu, Ruijin and Yuan, Zejian and Liu, Tie and Xiong, Zhiliang},
	year = {2021},
	keywords = {/unread},
	pages = {3693--3701},
}

@inproceedings{liu2021condlanenet,
	title = {{CondLaneNet}: a {Top}-to-down {Lane} {Detection} {Framework} {Based} on {Conditional} {Convolution}.},
	language = {en},
	booktitle = {{IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Liu, Lizhe and Chen, Xiaohao and Zhu, Siyu and Tan, Ping},
	year = {2021},
	keywords = {/unread},
	pages = {3753--3762},
}

@inproceedings{meng2021conditional,
	title = {Conditional {DETR} for {Fast} {Training} {Convergence}.},
	language = {en},
	booktitle = {{IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Meng, Depu and Chen, Xiaokang and Fan, Zejia and Zeng, Gang and Li, Houqiang and Yuan, Yuhui and Sun, Lei and Wang, Jingdong},
	year = {2021},
	keywords = {/unread},
	pages = {3631--3640},
}

@article{qin2024ultra,
	title = {Ultra {Fast} {Deep} {Lane} {Detection} with {Hybrid} {Anchor} {Driven} {Ordinal} {Classification}},
	language = {en},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Qin, Zequn and Zhang, Pengyi and Li, Xi},
	year = {2024},
	note = {Publisher: IEEE},
	keywords = {/unread},
	pages = {1--14},
}

@inproceedings{qu2021focus,
	title = {Focus on {Local}: {Detecting} {Lane} {Marker} {From} {Bottom} {Up} via {Key} {Point}.},
	language = {en},
	booktitle = {Computer {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Qu, Zhan and Jin, Huan and Zhou, Yang and Yang, Zhen and Zhang, Wei},
	year = {2021},
	keywords = {/unread},
	pages = {14122--14130},
}

@inproceedings{su2021structure,
	title = {Structure {Guided} {Lane} {Detection}.},
	language = {en},
	booktitle = {International {Joint} {Conference} on {Artificial} {Intelligence} ({IJCAI})},
	author = {Su, Jinming and Chen, Chao and Zhang, Ke and Luo, Junfeng and Wei, Xiaoming and Wei, Xiaolin},
	year = {2021},
	keywords = {/unread},
	pages = {997--1003},
}

@inproceedings{torres2020polylanenet,
	title = {{PolyLaneNet}: {Lane} {Estimation} via {Deep} {Polynomial} {Regression}.},
	language = {en},
	booktitle = {International {Conference} on {Pattern} {Recognition} ({ICPR})},
	author = {Torres, Lucas Tabelini and Berriel, Rodrigo Ferreira and Paixão, Thiago M. and Badue, Claudine and Souza, Alberto F. De and Oliveira-Santos, Thiago},
	year = {2020},
	keywords = {/unread},
	pages = {6150--6156},
}

@inproceedings{torres2021keep,
	title = {Keep {Your} {Eyes} on the {Lane}: {Real}-{Time} {Attention}-{Guided} {Lane} {Detection}.},
	language = {en},
	booktitle = {Computer {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Torres, Lucas Tabelini and Berriel, Rodrigo Ferreira and Paixão, Thiago M. and Badue, Claudine and Souza, Alberto F. De and Oliveira-Santos, Thiago},
	year = {2021},
	keywords = {/unread},
	pages = {294--302},
}

@inproceedings{wangVideoInstanceLane2022,
	address = {Lisboa Portugal},
	title = {Video {Instance} {Lane} {Detection} via {Deep} {Temporal} and {Geometry} {Consistency} {Constraints}},
	isbn = {978-1-4503-9203-7},
	url = {https://dl.acm.org/doi/10.1145/3503161.3547914},
	doi = {10.1145/3503161.3547914},
	abstract = {Video instance lane detection is one of the most important tasks in autonomous driving. Due to the very sparse region and weak context in lane annotations, accurately detecting instance-level lanes in real-world traffic scenarios is challenging, especially for scenes with occlusion, bad weather conditions, dim or dazzling lights. Current methods mainly address this problem by integrating features of adjacent video frames to simply encourage temporal constancy for image-level lane detectors. However, most of them ignore lane shape constraint of adjacent frames and geometry consistency of individual lanes, thereby harming the performance of video instance lane detection. In this paper, we propose TGC-Net via temporal and geometry consistency constraints for reliable video instance lane detection. Specifically, we devise a temporal recurrent feature-shift aggregation module (T-RESA) to learn spatio-temporal lane features along horizontal, vertical, and temporal directions of the feature tensor. We further impose temporal consistency constraint by encouraging spatial distribution consistency among the lane features of adjacent frames. Besides, we devise two effective geometry constraints to ensure the integrity and continuity of lane predictions by leveraging pairwise point affinity loss and vanishing point guided geometric context, respectively. Extensive experiments on public benchmark dataset show that our TGC-Net quantitatively and qualitatively outperforms state-of-the-art video instance lane detectors and video object segmentation competitors. Our code and our results have been released at https://github.com/wmq12345/TGC-Net.},
	language = {en},
	urldate = {2024-05-22},
	booktitle = {Proceedings of the 30th {ACM} {International} {Conference} on {Multimedia}},
	publisher = {ACM},
	author = {Wang, Mingqian and Zhang, Yujun and Feng, Wei and Zhu, Lei and Wang, Song},
	month = oct,
	year = {2022},
	keywords = {/unread},
	pages = {2324--2332},
	file = {Wang 等 - 2022 - Video Instance Lane Detection via Deep Temporal an.pdf:D\:\\bw_ch\\Zotero\\storage\\B4LAPIIZ\\Wang 等 - 2022 - Video Instance Lane Detection via Deep Temporal an.pdf:application/pdf},
}

@article{wang2018lanenet,
	title = {{LaneNet}: {Real}-{Time} {Lane} {Detection} {Networks} for {Autonomous} {Driving}},
	volume = {abs/1807.01726},
	language = {en},
	journal = {ArXiv},
	author = {Wang, Ze and Ren, Weiqiang and Qiu, Qiang},
	year = {2018},
	keywords = {/unread},
}

@inproceedings{xiao2023adnet,
	title = {{ADNet}: {Lane} {Shape} {Prediction} via {Anchor} {Decomposition}.},
	language = {en},
	booktitle = {{IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Xiao, Lingyu and Li, Xiang and Yang, Sen and Yang, Wankou},
	year = {2023},
	keywords = {/unread},
	pages = {6381--6390},
}

@inproceedings{xu2020curvelanenas,
	title = {{CurveLane}-{NAS}: {Unifying} {Lane}-{Sensitive} {Architecture} {Search} and {Adaptive} {Point} {Blending}.},
	language = {en},
	booktitle = {European {Conference} on {Computer} {Vision} ({ECCV})},
	author = {Xu, Hang and Wang, Shaoju and Cai, Xinyue and Zhang, Wei and Liang, Xiaodan and Li, Zhenguo},
	year = {2020},
	keywords = {/unread},
	pages = {689--704},
}

@inproceedings{zheng2021resa,
	title = {{RESA}: {Recurrent} {Feature}-{Shift} {Aggregator} for {Lane} {Detection}.},
	language = {en},
	booktitle = {{AAAI} {Conference} on {Artificial} {Intelligence} ({AAAI})},
	author = {Zheng, Tu and Fang, Hao and Zhang, Yi and Tang, Wenjian and Yang, Zheng and Liu, Haifeng and Cai, Deng},
	year = {2021},
	keywords = {/unread},
	pages = {3547--3554},
}

@article{DuoBuWeiLianHeYinFaZhiNengQiCheChuangXinFaZhanZhanLue2020,
	title = {多部委联合印发《智能汽车创新发展战略》},
	issn = {1004-1230},
	url = {https://kns.cnki.net/kcms2/article/abstract?v=axnrJTP8flxDSIHEp2Bjz0n0yD1AZ4lFNzEgxxHz6KGMGe9V8uKz3_lZsE-wgTm1ls-6-4BtK9B9hKWgDiEQVva8K_-zYoBW7qlR_ywg0zMEXctjDpkNuHX0dBdlNj9vzOrvUYkSztSeSNzdr25taw==&uniplatform=NZKPT&language=CHS},
	abstract = {{\textless}正{\textgreater}为深入贯彻落实党中央、国务院重要部署,顺应新一轮科技革命和产业变革趋势,抓住产业智能化发展战略机遇,加快推进智能汽车创新发展,国家发展改革委、中央网信办、科技部、工业和信息化部、公安部、财政部、自然资源部、住房城乡建设部、交通运输部、商务部、市场监管总局联合制定了《智能汽车创新发展战略》。现印发你们,请结合实际制定促进智能汽车创新发展的政策措施,着力推动各项战略任务有效落实。全文如下:智能汽车创新发展战略当今世界正经历百年未有之大},
	language = {zh},
	number = {1},
	urldate = {2024-05-22},
	journal = {广西节能},
	year = {2020},
	keywords = {《智能汽车创新发展战略》, 创新发展战略, 汽车强国, 智能汽车},
	pages = {8--10},
	file = {2020_多部委联合印发《智能汽车创新发展战略》.pdf:D\:\\bw_ch\\Zotero\\storage\\SWY9U7MH\\2020_多部委联合印发《智能汽车创新发展战略》.pdf:application/pdf},
}

@article{QiCheJiaShiZiDongHuaFenJiBiaoZhunFaBu2020,
	title = {《汽车驾驶自动化分级》标准发布},
	issn = {1004-6437},
	url = {https://kns.cnki.net/kcms2/article/abstract?v=axnrJTP8flwZhRCxXqkOB5dEvwlMPGSBvq_5EllQswtjz7I2zMBWu9SAcXCFZ967dPasB8WKA43jVFakUvcnaa3LIueV4Ap0hnaDgpGLkBwU-rZJ3_eooGQjTqxpzSejbuxj4Pj5QtonWM5YSx8FtQ==&uniplatform=NZKPT&language=CHS},
	abstract = {{\textless}正{\textgreater}根据国家标准化管理委员会下达的国家标准制修订计划,工信部于3月9日发布《汽车驾驶自动化分级》(报批稿)(以下简称《分级》)推荐性国家标准,将于2021年1月1日正式实施。这意味着我国即将正式拥有自己的自动驾驶汽车分级标准,也将为我国后续自动驾驶相关法律、法规、强制性标准的出台提供支撑。《分级》对驾驶自动化、驾驶自动化系统、驾驶自动化功能、车辆横向运动控制、目标和事件探测与响应、动态驾驶任务接管、设计运行范围、接管请求等术语定义进},
	language = {zh},
	number = {2},
	urldate = {2024-05-22},
	journal = {机器人技术与应用},
	year = {2020},
	keywords = {/unread, 《汽车驾驶自动化分级》, 驾驶辅助, 汽车驾驶},
	pages = {3},
	file = {2020_《汽车驾驶自动化分级》标准发布.pdf:D\:\\bw_ch\\Zotero\\storage\\45A9SJ3R\\2020_《汽车驾驶自动化分级》标准发布.pdf:application/pdf},
}

@article{chen2023bsnet,
	title = {{BSNet}: {Lane} {Detection} via {Draw} {B}-spline {Curves} {Nearby}},
	volume = {abs/2301.06910},
	language = {en},
	journal = {ArXiv},
	author = {Chen, Haoxin and Wang, Mengmeng and Liu, Yong},
	year = {2023},
	keywords = {/unread},
}

@misc{alabaComprehensiveSurveyDeep2022,
	title = {A {Comprehensive} {Survey} of {Deep} {Learning} {Multisensor} {Fusion}-based {3D} {Object} {Detection} for {Autonomous} {Driving}: {Methods}, {Challenges}, {Open} {Issues}, and {Future} {Directions}},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	shorttitle = {A {Comprehensive} {Survey} of {Deep} {Learning} {Multisensor} {Fusion}-based {3D} {Object} {Detection} for {Autonomous} {Driving}},
	url = {https://www.techrxiv.org/doi/full/10.36227/techrxiv.20443107.v1},
	doi = {10.36227/techrxiv.20443107.v1},
	abstract = {Autonomous driving requires accurate, robust, and fast decision-making perception systems to understand the driving environment. Object detection is critical in allowing the perception system to understand the environment. The perception systems, especially 2D object detection and classiﬁcation, have succeeded because of the emergence of deep learning (DL) in computer vision (CV) applications. However, 2D object detection lacks depth information, which is crucial to understanding the driving environment. Therefore, 3D object detection is fundamental for the perception system of autonomous driving and robotics applications to estimate the objects’ location and understand the driving environment. The CV community has been giving much attention recently to 3D object detection because of the growth of DL models and the need to know accurate locations of objects.},
	language = {en},
	urldate = {2024-05-22},
	author = {Alaba, Simegnew},
	month = aug,
	year = {2022},
	file = {Alaba - 2022 - A Comprehensive Survey of Deep Learning Multisenso.pdf:D\:\\bw_ch\\Zotero\\storage\\BZS8GQDS\\Alaba - 2022 - A Comprehensive Survey of Deep Learning Multisenso.pdf:application/pdf},
}

@article{abualsaud2021laneaf,
	title = {{LaneAF}: {Robust} {Multi}-{Lane} {Detection} {With} {Affinity} {Fields}.},
	volume = {6},
	language = {en},
	number = {4},
	journal = {IEEE Robotics and Automation Letters},
	author = {Abualsaud, Hala and Liu, Sean and Lu, David and Situ, Kenny and Rangesh, Akshay and Trivedi, Mohan M.},
	year = {2021},
	keywords = {/unread},
	pages = {7477--7484},
}

@article{chenMilestonesAutonomousDriving2023,
	title = {Milestones in {Autonomous} {Driving} and {Intelligent} {Vehicles}—{Part} {II}: {Perception} and {Planning}},
	volume = {53},
	issn = {2168-2232},
	shorttitle = {Milestones in {Autonomous} {Driving} and {Intelligent} {Vehicles}—{Part} {II}},
	url = {https://ieeexplore.ieee.org/abstract/document/10156892},
	doi = {10.1109/TSMC.2023.3283021},
	abstract = {A growing interest in autonomous driving (AD) and intelligent vehicles (IVs) is fueled by their promise for enhanced safety, efficiency, and economic benefits. While previous surveys have captured progress in this field, a comprehensive and forward-looking summary is needed. Our work fills this gap through three distinct articles. The first part, a “survey of surveys” (SoS), outlines the history, surveys, ethics, and future directions of AD and IV technologies. The second part, “Milestones in AD and IVs Part I: Control, Computing System Design, Communication, high-definition map (HD map), Testing, and Human Behaviors” delves into the development of control, computing system, communication, HD map, testing, and human behaviors in IVs. This part, the third part, reviews perception and planning in the context of IVs. Aiming to provide a comprehensive overview of the latest advancements in AD and IVs, this work caters to both newcomers and seasoned researchers. By integrating the SoS and Part I, we offer unique insights and strive to serve as a bridge between past achievements and future possibilities in this dynamic field.},
	language = {en},
	number = {10},
	urldate = {2024-05-22},
	journal = {IEEE Transactions on Systems, Man, and Cybernetics: Systems},
	author = {Chen, Long and Teng, Siyu and Li, Bai and Na, Xiaoxiang and Li, Yuchen and Li, Zixuan and Wang, Jinjun and Cao, Dongpu and Zheng, Nanning and Wang, Fei-Yue},
	month = oct,
	year = {2023},
	note = {Conference Name: IEEE Transactions on Systems, Man, and Cybernetics: Systems},
	keywords = {/unread, Location awareness, Visualization, Autonomous driving (AD), communication, control, high-definition map (HD map), human behaviors, intelligent vehicles (IVs), Laser radar, perception, planning, Planning, Simultaneous localization and mapping, survey of surveys (SoS), Surveys, system design, testing, Testing},
	pages = {6401--6415},
	file = {Chen et al_2023_Milestones in Autonomous Driving and Intelligent Vehicles—Part II.pdf:D\:\\bw_ch\\Zotero\\storage\\DTZCG9CY\\Chen et al_2023_Milestones in Autonomous Driving and Intelligent Vehicles—Part II.pdf:application/pdf;IEEE Xplore Abstract Record:D\:\\bw_ch\\Zotero\\storage\\P72HDBF8\\10156892.html:text/html},
}
